{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19517291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import googleapiclient.discovery as gapi\n",
    "from googleapiclient.errors import HttpError\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import concurrent.futures\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acae34e",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c1d1518",
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_FOLDER = 'C:\\\\Coding Projects\\\\YoutubeCookingData\\\\'\n",
    "\n",
    "with open(CURRENT_FOLDER + \"apiKeys.txt\") as f:\n",
    "    YOUTUBE_API_KEY = f.read()\n",
    "youtube_service = gapi.build('youtube', 'v3', developerKey=YOUTUBE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae3b28a",
   "metadata": {},
   "source": [
    "# Final Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "131be543",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "\n",
    "def jsonFieldHandler(json, key1, key2='', key3='', key4=''):\n",
    "    # If the key doesn't exist, return ''\n",
    "    try:\n",
    "        if(key1 and key2 and key3 and key4):\n",
    "            return json[key1][key2][key3][key4]\n",
    "        elif(key1 and key2 and key3):\n",
    "            return json[key1][key2][key3]\n",
    "        elif(key1 and key2):\n",
    "            return json[key1][key2]\n",
    "        elif(key1):\n",
    "            return json[key1]\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "# Gets the data for all channels and parses it\n",
    "def getChannelDataHandler(channel_id_list):\n",
    "    channel_dict_list = []\n",
    "    channel_response_json = []\n",
    "    \n",
    "    # Get all fields for Channel\n",
    "    for i in range(0, len(channel_id_list), 50):\n",
    "        channel_response_json += getChannelData(channel_id_list[i:i+50])\n",
    "        \n",
    "    # Parse response into dictionary\n",
    "    for result in channel_response_json:\n",
    "#         print(f\"LOG: Fetching Data for Channel ID {result['id']}\")\n",
    "        channel_dict = {}\n",
    "        \n",
    "        channel_dict['Channel ID'] = jsonFieldHandler(result, 'id')\n",
    "        channel_dict['Title'] = jsonFieldHandler(result, 'snippet','title')\n",
    "        channel_dict['Description'] = jsonFieldHandler(result, 'snippet','description')\n",
    "        channel_dict['URL'] = jsonFieldHandler(result, 'snippet','customUrl')\n",
    "        channel_dict['Channel Created Date'] = jsonFieldHandler(result, 'snippet','publishedAt')\n",
    "        channel_dict['Thumbnail URL'] = jsonFieldHandler(result, 'snippet', 'thumbnails', 'high', 'url')\n",
    "        channel_dict['Language'] = jsonFieldHandler(result, 'snippet', 'defaultLanguage')\n",
    "        channel_dict['Country'] = jsonFieldHandler(result, 'snippet', 'country')\n",
    "        channel_dict['Views'] = jsonFieldHandler(result, 'statistics', 'viewCount')\n",
    "        channel_dict['Subscriber Count'] = jsonFieldHandler(result, 'statistics', 'subscriberCount')\n",
    "        channel_dict['Video Count'] = jsonFieldHandler(result, 'statistics', 'videoCount')\n",
    "        channel_dict['Topics'] = jsonFieldHandler(result, 'topicDetails', 'topicIds')\n",
    "        channel_dict['Topic Categories'] = jsonFieldHandler(result, 'topicDetails', 'topicCategories')\n",
    "        channel_dict['Upload Playlist ID'] = jsonFieldHandler(result, 'contentDetails', 'relatedPlaylists', 'uploads')\n",
    "\n",
    "        channel_dict_list.append(channel_dict)\n",
    "        \n",
    "    return pd.DataFrame(channel_dict_list)\n",
    "\n",
    "# Gets all fields for a given list of channel\n",
    "def getChannelData(channel_id_list):\n",
    "    request = youtube_service.channels().list(\n",
    "        part=['id', 'snippet', 'statistics', 'topicDetails', 'contentDetails'],\n",
    "        id=channel_id_list,\n",
    "        maxResults = 50\n",
    "    )\n",
    "    \n",
    "    results = request.execute()\n",
    "    return results['items']\n",
    "\n",
    "# Returns the upload playlist for a list of channel ids\n",
    "def getUploadPlaylistsforChannelHandler(channel_id_list):\n",
    "    channel_response_json = []\n",
    "    upload_playlist_list = []\n",
    "    \n",
    "    # Get upload playlists\n",
    "    for i in range(0, len(channel_id_list), 50):\n",
    "        channel_response_json += getUploadPlaylistsforChannel(channel_id_list[i:i+50])\n",
    "    \n",
    "    # Get only the channel's upload playlist id\n",
    "    return [jsonFieldHandler(channel_json, 'contentDetails', 'relatedPlaylists', 'uploads') for channel_json in channel_response_json]\n",
    "\n",
    "# Gets upload playlist ID for a list of channel IDs\n",
    "def getUploadPlaylistsforChannel(channel_id_list):\n",
    "    request = youtube_service.channels().list(\n",
    "    part=['contentDetails'],\n",
    "        id=channel_id_list,\n",
    "        maxResults=50\n",
    "    )\n",
    "    \n",
    "    result = request.execute()\n",
    "    \n",
    "    return result['items']\n",
    "    \n",
    "    \n",
    "    \n",
    "# Function that returns all videoIds for a given channel's upload playlist\n",
    "def getVideoListForPlaylist(upload_playlist_id, page_token=''):\n",
    "    video_ids = []\n",
    "\n",
    "    t1 = time.time()\n",
    "    \n",
    "    request = youtube_service.playlistItems().list(\n",
    "        part=\"contentDetails\",\n",
    "        playlistId=upload_playlist_id,\n",
    "        maxResults=50,\n",
    "        pageToken=page_token\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        results = request.execute()\n",
    "        result_videos = results['items']\n",
    "    except HttpError as err:\n",
    "        appendToLog(HttpError)\n",
    "        raise err\n",
    "    \n",
    "    # If there is a next page then append videos to result array\n",
    "    if('nextPageToken' in results):\n",
    "        nextPage = results['nextPageToken']\n",
    "        result_videos += getVideoListForPlaylist(upload_playlist_id, nextPage)\n",
    "    \n",
    "    # If not called by another instance of this function, return only IDs\n",
    "    if(page_token == ''):\n",
    "#         print(f\"LOG: Working on Channel ID {channelId}.\")\n",
    "        t2 = time.time()\n",
    "        appendToLog(f'Time elapsed for videos of playlist {upload_playlist_id} {t2-t1} for {len(result_videos)} videos')\n",
    "        return [vid['contentDetails']['videoId'] for vid in result_videos]\n",
    "    else:\n",
    "        # If recursing, return whole response list\n",
    "        return result_videos\n",
    "\n",
    "# Function that parses the return JSON from getVideoListForPlaylist and gets additional columns\n",
    "def getVideoDataHandler(video_id_list):\n",
    "    video_response_json = []\n",
    "    video_dict_list = []\n",
    "    \n",
    "    # Get all fields for Video\n",
    "    for i in range(0, len(video_id_list), 50):\n",
    "        try:\n",
    "            video_response_json += getVideoData(video_id_list[i:i+50])\n",
    "        except HttpError as err:\n",
    "            raise err\n",
    "    \n",
    "    # Parse response into dictionary\n",
    "    for result in video_response_json:\n",
    "#         print(f\"\\tLOG: Fetching Data for Video ID {result['id']}\")\n",
    "        video_dict = {}\n",
    "        \n",
    "        video_dict['Title'] = jsonFieldHandler(result, 'snippet', 'title')\n",
    "        video_dict['Video ID'] = jsonFieldHandler(result, 'id')\n",
    "        video_dict['Channel ID'] = jsonFieldHandler(result, 'snippet', 'channelId')\n",
    "        video_dict['Duration'] = jsonFieldHandler(result, 'contentDetails', 'duration')\n",
    "        video_dict['Description'] = jsonFieldHandler(result, 'snippet', 'description')\n",
    "        video_dict['Publish Date'] = jsonFieldHandler(result, 'snippet', 'publishedAt')\n",
    "        video_dict['Thumbnail URL'] = jsonFieldHandler(result, 'snippet', 'thumbnails', 'high',  'url')\n",
    "        video_dict['View Count'] = jsonFieldHandler(result, 'statistics', 'viewCount')\n",
    "        video_dict['Like Count'] = jsonFieldHandler(result, 'statistics', 'likeCount')\n",
    "        video_dict['Comment Count'] = jsonFieldHandler(result, 'statistics', 'commentCount')        \n",
    "        video_dict['Video Definition'] = jsonFieldHandler(result, 'contentDetails', 'definition')\n",
    "        video_dict['Default Audio Language'] = jsonFieldHandler(result, 'snippet', 'defaultAudioLanguage')\n",
    "        video_dict['Tags'] = jsonFieldHandler(result, 'snippet', 'tags')\n",
    "        video_dict['Category ID'] = jsonFieldHandler(result, 'snippet', 'categoryId')\n",
    "        video_dict['Topic Details'] = jsonFieldHandler(result, 'topicDetails')\n",
    "        video_dict['Made for Kids'] = jsonFieldHandler(result, 'status', 'madeForKids')\n",
    "        video_dict['Favorite Count'] = jsonFieldHandler(result, 'statistics', 'favoriteCount')\n",
    "        video_dict_list.append(video_dict)\n",
    "        \n",
    "    return pd.DataFrame(video_dict_list)\n",
    "     \n",
    "def getVideoData(list_of_videos):\n",
    "    request = youtube_service.videos().list(\n",
    "        part=['contentDetails', 'liveStreamingDetails', 'id',\n",
    "              'snippet', 'statistics', 'status', 'topicDetails'],\n",
    "        id=list_of_videos,\n",
    "        maxResults=50,\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        results = request.execute()\n",
    "        return results['items']\n",
    "    except HttpError as err: \n",
    "        # If error, return \n",
    "        appendToLog(HttpError)\n",
    "        raise err\n",
    "\n",
    "# Gets the comment threads for all videos in a given list\n",
    "def getCommentThreadsHandler(list_of_videos):\n",
    "    dataframes_list = []\n",
    "    \n",
    "    for video_id in list_of_videos:\n",
    "        try:\n",
    "            dataframes_list.append(getCommentsForVideo(video_id))\n",
    "        except HttpError as err:\n",
    "            # If it is bad request, try again in 5 seconds. Else raise error\n",
    "            if(err.resp.status == 400):\n",
    "                try:\n",
    "                    appendToLog(f\"Potentially transient ({err.resp.status}), trying again for video {video_id}\")\n",
    "                    time.sleep(600)\n",
    "                    dataframes_list.append(getCommentsForVideo(video_id))\n",
    "                except HttpError as err:\n",
    "                    raise err\n",
    "            else:\n",
    "                raise err\n",
    "\n",
    "    return pd.concat(dataframes_list, ignore_index=True)\n",
    "\n",
    "# Get list of comment threads for a given video, returns a dataframe\n",
    "def getCommentsForVideo(video_id, page_token=\"\"):\n",
    "#     print(f\"\\tLOG: Fetching Comment Data for Video ID {video_id}, {page_token}\")\n",
    "    t1 = time.time()\n",
    "    request = youtube_service.commentThreads().list(\n",
    "        part=['id', 'replies', 'snippet'],\n",
    "        videoId=video_id,\n",
    "        maxResults=100,\n",
    "        pageToken=page_token\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        results = request.execute()\n",
    "        result_comments = results['items']\n",
    "    \n",
    "        if('nextPageToken' in results):\n",
    "            next_page = results['nextPageToken']\n",
    "            result_comments += getCommentsForVideo(video_id, next_page)\n",
    "        \n",
    "        # If not recursing, parse the full response\n",
    "        if(page_token == ''):\n",
    "            t2 = time.time()\n",
    "            appendToLog(f'Time elapsed for comments on video {video_id} {t2-t1} for {len(result_comments)} comments')\n",
    "            return parseCommentThreadResponse(result_comments)\n",
    "        else: \n",
    "            # Return raw results if called by another instance of this function\n",
    "            return result_comments\n",
    "    except HttpError as err:\n",
    "        appendToLog(f'{err.resp.status} - {err._get_reason()} - {video_id} with page {page_token}')\n",
    "        raise err\n",
    "\n",
    "# Parse comment threads response JSON\n",
    "def parseCommentThreadResponse(list_of_threads):\n",
    "    thread_dict_list = []\n",
    "    \n",
    "    for thread in list_of_threads:\n",
    "#         print(f\"\\t\\tLOG: Fetching Data for Comment Thread ID {thread['id']}\")\n",
    "        thread_dict = {}\n",
    "        \n",
    "        thread_dict['Comment Thread ID'] = jsonFieldHandler(thread, 'id')\n",
    "        thread_dict['Video ID'] = jsonFieldHandler(thread, 'snippet', 'videoId')\n",
    "        thread_dict['Top Level Comment'] = jsonFieldHandler(thread, 'snippet', 'topLevelComment')\n",
    "        thread_dict['Total Replies'] = jsonFieldHandler(thread, 'snippet', 'totalReplyCount')\n",
    "        thread_dict['Can Reply'] = jsonFieldHandler(thread, 'snippet', 'canReply')\n",
    "        thread_dict['Replies'] = jsonFieldHandler(thread, 'replies')\n",
    "        \n",
    "        thread_dict_list.append(thread_dict)\n",
    "    \n",
    "    return pd.DataFrame(thread_dict_list)\n",
    "\n",
    "def appendToLog(message): \n",
    "    print(message)\n",
    "    current_time = datetime.datetime.now()\n",
    "    \n",
    "    file_path = CURRENT_FOLDER + f\"Log {current_time.year}-{current_time.month}-{current_time.day}.txt\"\n",
    "\n",
    "    if(os.path.exists(file_path)):\n",
    "        append_write = 'a'\n",
    "    else:\n",
    "        append_write ='w'\n",
    "    \n",
    "    with open(file_path, append_write) as f:\n",
    "        f.write(message + '\\n')\n",
    "\n",
    "def main(channels_flag=True, videos_flag=True, comments_flag=True, current_folder=''):\n",
    "    start_time = time.time()\n",
    "    appendToLog(f\"Starting execution for channel data: {channels_flag}, video data: {videos_flag}, comment data: {comments_flag}\")\n",
    "    CHANNEL_ID_TOTAL_DF = pd.read_csv(current_folder + \"Channel IDs.csv\")\n",
    "\n",
    "    # File Paths\n",
    "    channels_file_path = current_folder + \"Channel Data.csv\"\n",
    "    videos_file_path = current_folder + \"Video Data.csv\"\n",
    "    comments_file_path = current_folder + \"Comment Data.csv\"\n",
    "    \n",
    "    # If Channels step is done, skip\n",
    "    if(channels_flag):\n",
    "        # If the file exists check how many channels still need to be queried, else get the whole list\n",
    "        if(os.path.exists(channels_file_path)):\n",
    "            # Get list of channel ids that have been queried and compare against the total list of ids\n",
    "            channel_data_done_ids = pd.read_csv(channels_file_path)['Channel ID'].to_list()\n",
    "            \n",
    "            channelIdsToFetch = CHANNEL_ID_TOTAL_DF[~CHANNEL_ID_TOTAL_DF['ID'].isin(channel_data_done_ids)]['ID'].to_list()\n",
    "            appendToLog(f\"{len(channel_data_done_ids)} channels already done out of {CHANNEL_ID_TOTAL_DF['ID'].shape[0]}\")\n",
    "        else:\n",
    "            channelIdsToFetch = CHANNEL_ID_TOTAL_DF['ID'].to_list()\n",
    "            appendToLog(f\"No channels already done, querying all {len(channelIdsToFetch)} IDs\")\n",
    "        \n",
    "        # If there are channels to fetch, get data and write to csv\n",
    "        if(len(channelIdsToFetch) > 0):\n",
    "            channels_df = getChannelDataHandler(channelIdsToFetch)\n",
    "            # If there is an existing file, concat to avoid overwriting\n",
    "            if(os.path.exists(channels_file_path)):\n",
    "                    channels_df = pd.concat([channels_df, pd.read_csv(channels_file_path)], ignore_index=True)\n",
    "            channels_df.to_csv(channels_file_path, index=False)\n",
    "            appendToLog(f\"Done {channels_df.shape[0]} channels\")\n",
    "    \n",
    "    # Check video progress, skip if done\n",
    "    if(videos_flag):\n",
    "        video_ids = []\n",
    "        videos_list = []\n",
    "        # Check if a given channel has already had its videos fetched\n",
    "        if(os.path.exists(videos_file_path)):\n",
    "            channel_data_done_ids_videos = pd.read_csv(videos_file_path)['Channel ID'].unique().tolist()\n",
    "            \n",
    "            channel_ids_to_fetch_videos = CHANNEL_ID_TOTAL_DF[~CHANNEL_ID_TOTAL_DF['ID'].isin(channel_data_done_ids_videos)]['ID'].to_list()\n",
    "            playlist_ids_to_fetch_videos = getUploadPlaylistsforChannelHandler(channel_ids_to_fetch_videos)\n",
    "            appendToLog(f\"{len(channel_data_done_ids_videos)} channel's videos already done out of {len(CHANNEL_ID_TOTAL_DF['ID'].to_list())}\")\n",
    "            # Read in current data\n",
    "            videos_list.append(pd.read_csv(videos_file_path))\n",
    "        else:\n",
    "            channel_ids_to_fetch_videos = CHANNEL_ID_TOTAL_DF['ID'].to_list()\n",
    "            appendToLog(f\"No channel's videos already done, querying all {len(channel_ids_to_fetch_videos)} channels\")\n",
    "            playlist_ids_to_fetch_videos = getUploadPlaylistsforChannelHandler(channel_ids_to_fetch_videos)\n",
    "        for playlist_id in playlist_ids_to_fetch_videos:\n",
    "            try:\n",
    "                video_ids = getVideoListForPlaylist(playlist_id)\n",
    "                videos_list.append(getVideoDataHandler(video_ids))\n",
    "            except HttpError as err:\n",
    "                appendToLog(f\"Error caught while fetching video data, ending execution\")\n",
    "                # Write current data to file\n",
    "                if(os.path.exists(videos_file_path)):\n",
    "                    videos_list += pd.read_csv(videos_file_path)\n",
    "                pd.concat(videos_list, ignore_index=True).to_csv(videos_file_path, index=False)\n",
    "                return  \n",
    "\n",
    "        # If all videos are read successfully, write to file and continue\n",
    "        if(len(channel_ids_to_fetch_videos) > 0):\n",
    "            if(os.path.exists(videos_file_path)):\n",
    "                        videos_list += pd.read_csv(videos_file_path)\n",
    "            pd.concat(videos_list, ignore_index=True).to_csv(videos_file_path, index=False)\n",
    "            \n",
    "    if(comments_flag):\n",
    "        comment_ids = []\n",
    "        comments_list = []\n",
    "        if(os.path.exists(videos_file_path)):\n",
    "            videos_data_total_ids_df = pd.read_csv(videos_file_path)\n",
    "            if(os.path.exists(comments_file_path)):\n",
    "                videos_data_done_ids_comments = pd.read_csv(comments_file_path)['Video ID'].unique().tolist()\n",
    "                # Only fetch videos that have comments\n",
    "                video_ids_to_fetch_comments = videos_data_total_ids_df[~videos_data_total_ids_df['Video ID'].isin(videos_data_done_ids_comments) & videos_data_total_ids_df['Comment Count'] > 0]['Video ID'].to_list()\n",
    "                appendToLog(f\"Done {len(videos_data_done_ids_comments)} video's comments out of {videos_data_total_ids_df.shape[0]}\")\n",
    "                # Read in current data\n",
    "                comments_list.append(pd.read_csv(comments_file_path))\n",
    "            else:\n",
    "                # Only fetch videos that have comments\n",
    "                video_ids_to_fetch_comments = videos_data_total_ids_df[videos_data_total_ids_df['Comment Count'] > 0]['Video ID'].to_list()\n",
    "                appendToLog(f\"No comments done, getting data for all {len(video_ids_to_fetch_comments)} videos\")\n",
    "        else:\n",
    "            appendToLog(\"No video data file, exiting\")\n",
    "            return\n",
    "        for video_id in video_ids_to_fetch_comments:\n",
    "            try:\n",
    "                comments_list.append(getCommentThreadsHandler([video_id]))\n",
    "            except HttpError as err:\n",
    "                appendToLog(f\"Error caught while fetching comment data, ending execution\")\n",
    "                # Write current data to file \n",
    "                if(os.path.exists(comments_file_path)):\n",
    "                    comments_list += pd.read_csv(comments_file_path)\n",
    "                pd.concat(comments_list, ignore_index=True).to_csv(comments_file_path,index=False)\n",
    "                return\n",
    "        # Write each video's comments to the file if not excepted\n",
    "        if(len(video_ids_to_fetch_comments) > 0):\n",
    "            if(os.path.exists(comments_file_path)):\n",
    "                comments_list += pd.read_csv(comments_file_path)\n",
    "            pd.concat(comments_list, ignore_index=True).to_csv(comments_file_path, index=False)\n",
    "    end_time = time.time()\n",
    "    appendToLog(f\"Finished data gathering. Overall execution time: {end_time - start_time}\")\n",
    "        \n",
    "    return\n",
    "    \n",
    "# Takes bulk comment data and splits into subsheets, one for each channel\n",
    "def postProcessingComments(filePath, targetPath):\n",
    "    comment_data = pd.read_csv(\"Comment Data.csv\")\n",
    "    appendToLog(f\"Beginning comment post processing for {comment_data.shape[0]} comment threads.\")\n",
    "    videos_channels = pd.read_csv(\"Video Data.csv\")\n",
    "    video_channels = videos_channels[[\"Video ID\", \"Channel ID\"]]\n",
    "\n",
    "    for channelId in video_channels['Channel ID'].unique():\n",
    "        temp_df = comment_data[comment_data['Video ID'].isin(video_channels[video_channels['Channel ID'] == channelId]['Video ID'])]\n",
    "\n",
    "        # If there was more than one comment thread found for the channel\n",
    "        if(temp_df.shape[0] > 1):\n",
    "            file_path = f\"{targetPath}{channelId}_comment_data.csv\"\n",
    "            # If the file exists, then read the current data drop duplicates and save with the new data. Else just create the file\n",
    "            if(os.path.exists(file_path)):\n",
    "                pd.concat([temp_df, pd.read_csv(file_path)], ignore_index=True).drop_duplicates().to_csv(file_path, index=False)\n",
    "            else:\n",
    "                temp_df.drop_duplicates().to_csv(file_path, index=False)\n",
    "            appendToLog(f\"Saved comment data for {channelId} in {file_path}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d253fd38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cc2eed45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Comment Post Processing for 661783 comment threads.\n",
      "Saved comment data for UCN1h109PDDp_wYIFsoWmZrQ in C:\\Coding Projects\\YoutubeCookingData\\Comment Data\\UCN1h109PDDp_wYIFsoWmZrQ_comment_data.csv\n",
      "Saved comment data for UCRIZtPl9nb9RiXc9btSTQNw in C:\\Coding Projects\\YoutubeCookingData\\Comment Data\\UCRIZtPl9nb9RiXc9btSTQNw_comment_data.csv\n"
     ]
    }
   ],
   "source": [
    "main(True, True, True, current_folder=CURRENT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff587cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "postProcessingComments(\"Comment Data.csv\", CURRENT_FOLDER + \"Comment Data\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ddc353",
   "metadata": {},
   "source": [
    "# Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0c12f03c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "661783"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(r\"Comment Data\\UCN1h109PDDp_wYIFsoWmZrQ_comment_data.csv\").shape[0] + pd.read_csv(r\"Comment Data\\UCRIZtPl9nb9RiXc9btSTQNw_comment_data.csv\").shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "418cd974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UCN1h109PDDp_wYIFsoWmZrQ':                  Comment Thread ID     Video ID  \\\n",
       " 0       Ugw5kDJmxc1Ty_3Gmrh4AaABAg  v-x6rJiva5A   \n",
       " 1       Ugytrgb7SwmfQV9bP894AaABAg  v-x6rJiva5A   \n",
       " 2       UgwdkeCC0d7q53Xjkv94AaABAg  v-x6rJiva5A   \n",
       " 3       UgxzhoyouIbDKYB1zsd4AaABAg  v-x6rJiva5A   \n",
       " 4       UgxdyVg2AdekoSODZZJ4AaABAg  v-x6rJiva5A   \n",
       " ...                            ...          ...   \n",
       " 132614  UgySZlvG29LlBYrbamV4AaABAg  JILCA27avnw   \n",
       " 132615  UgxfKBVRMC88oPw6D7J4AaABAg  JILCA27avnw   \n",
       " 132616  Ugyi-Wb-OaUBevJ7y194AaABAg  JILCA27avnw   \n",
       " 132617  UgyVGC-7bdKZ2DWvJyh4AaABAg  JILCA27avnw   \n",
       " 132618        UgjDIhTPvKJL93gCoAEC  JILCA27avnw   \n",
       " \n",
       "                                         Top Level Comment  Total Replies  \\\n",
       " 0       {'kind': 'youtube#comment', 'etag': '-X0m34kar...              0   \n",
       " 1       {'kind': 'youtube#comment', 'etag': 'a-AQvjIwO...              0   \n",
       " 2       {'kind': 'youtube#comment', 'etag': 'reFIe_4MQ...              0   \n",
       " 3       {'kind': 'youtube#comment', 'etag': 'xa-GIGGdE...              0   \n",
       " 4       {'kind': 'youtube#comment', 'etag': 'vFUu-O1oG...              0   \n",
       " ...                                                   ...            ...   \n",
       " 132614  {'kind': 'youtube#comment', 'etag': '1QydK-fu1...              0   \n",
       " 132615  {'kind': 'youtube#comment', 'etag': 'tY-1D1xJp...              1   \n",
       " 132616  {'kind': 'youtube#comment', 'etag': 'tKHRUOLCR...              0   \n",
       " 132617  {'kind': 'youtube#comment', 'etag': 'ZcbE2i6H6...              0   \n",
       " 132618  {'kind': 'youtube#comment', 'etag': '2WAjVWbty...              0   \n",
       " \n",
       "         Can Reply                                            Replies  \n",
       " 0            True                                                NaN  \n",
       " 1            True                                                NaN  \n",
       " 2            True                                                NaN  \n",
       " 3            True                                                NaN  \n",
       " 4            True                                                NaN  \n",
       " ...           ...                                                ...  \n",
       " 132614       True                                                NaN  \n",
       " 132615       True  {'comments': [{'kind': 'youtube#comment', 'eta...  \n",
       " 132616       True                                                NaN  \n",
       " 132617       True                                                NaN  \n",
       " 132618       True                                                NaN  \n",
       " \n",
       " [132619 rows x 6 columns],\n",
       " 'UCRIZtPl9nb9RiXc9btSTQNw':                  Comment Thread ID     Video ID  \\\n",
       " 132619  UgzyyJ23F3qJdntryiB4AaABAg  _ENF7QrrCtA   \n",
       " 132620  Ugx8K7N8Fqwka-mIz9V4AaABAg  _ENF7QrrCtA   \n",
       " 132621  UgxlZ42VMugMURh6a3x4AaABAg  _ENF7QrrCtA   \n",
       " 132622  UgzhiBfudUztlgftpgx4AaABAg  _ENF7QrrCtA   \n",
       " 132623  Ugyvj0yfef7ILTBLkRF4AaABAg  _ENF7QrrCtA   \n",
       " ...                            ...          ...   \n",
       " 661778        Ugg8b8nRySkZxXgCoAEC  GxnKz1oVpyE   \n",
       " 661779        UgjSI4R-TldsqXgCoAEC  GxnKz1oVpyE   \n",
       " 661780        UgjuoLFt9YawtngCoAEC  GxnKz1oVpyE   \n",
       " 661781        UggexPSJaQ_eU3gCoAEC  GxnKz1oVpyE   \n",
       " 661782        UggNp_0wkETa4ngCoAEC  GxnKz1oVpyE   \n",
       " \n",
       "                                         Top Level Comment  Total Replies  \\\n",
       " 132619  {'kind': 'youtube#comment', 'etag': 'AqKZsK_4X...              0   \n",
       " 132620  {'kind': 'youtube#comment', 'etag': 'io7MB65_X...              0   \n",
       " 132621  {'kind': 'youtube#comment', 'etag': 'Zf1p9wjEr...              0   \n",
       " 132622  {'kind': 'youtube#comment', 'etag': 'l6equfMEu...              0   \n",
       " 132623  {'kind': 'youtube#comment', 'etag': 'xqbc_dgu5...              0   \n",
       " ...                                                   ...            ...   \n",
       " 661778  {'kind': 'youtube#comment', 'etag': 'do39CTtx1...              0   \n",
       " 661779  {'kind': 'youtube#comment', 'etag': 'f1BxoX5hZ...              2   \n",
       " 661780  {'kind': 'youtube#comment', 'etag': '0e-NS6Mc3...              0   \n",
       " 661781  {'kind': 'youtube#comment', 'etag': 'GSU7Eyp0j...              0   \n",
       " 661782  {'kind': 'youtube#comment', 'etag': 'MHHRnsVmb...              0   \n",
       " \n",
       "         Can Reply                                            Replies  \n",
       " 132619       True                                                NaN  \n",
       " 132620       True                                                NaN  \n",
       " 132621       True                                                NaN  \n",
       " 132622       True                                                NaN  \n",
       " 132623       True                                                NaN  \n",
       " ...           ...                                                ...  \n",
       " 661778       True                                                NaN  \n",
       " 661779       True  {'comments': [{'kind': 'youtube#comment', 'eta...  \n",
       " 661780       True                                                NaN  \n",
       " 661781       True                                                NaN  \n",
       " 661782       True                                                NaN  \n",
       " \n",
       " [529164 rows x 6 columns],\n",
       " 'UCDq5v10l4wkV5-ZBIJJFbzQ': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UCMmZEL8jV1B61NKAXcyW87A': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UCsWpnu6EwIYDvlHoOESpwYg': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UCQOQ3RxX_o-B-68wSKdcfMQ': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UCTVR5DSxWPpAVI8TzaaXRqQ': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UCvw6Y1kr_8bp6B5m1dqNyiw': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UCekQr9znsk2vWxBo3YiLq2w': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UCn5fhcGRrCvrmFibPbT6q1A': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UCIEv3lZ_tNXHzL3ox-_uUGQ': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UCzH5n3Ih5kgQoiDAQt2FwLw': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UCJHA_jMfCvEnv-3kRjTCQXw': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UCVVAnxQ2YMC_qlc7QfPA2YQ': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UCJFp8uSYCjXOMnkUyb3CQ3Q': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UCfyehHM_eo4g5JUyWmms2LA': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UCRPWH2YmwgbVGz6zJZG1afA': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UCl0kP-Cfe-GGic7Ilnk-u_Q': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UCJQL1Fai-9GlVunsbP4x8Pg': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UCmoX4QULJ9MB00xW4coMiOw': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UCqqJQ_cXSat0KIAVfIfKkVA': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UCbpMy0Fg74eXXkvxJrtEn3w': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UCbRj3Tcy1Zoz3rcf83nW5kw': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UC9_p50tH3WmMslWRWKnM7dQ': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UCAzyupPG4vdo8jd9nJ13LAw': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UCciOWSHoC_UFmInZBlVHSuA': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UCD5WWnRed32y3xGwmrDhUiQ': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UC58sWqKKNBQDphes0Ys037w': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UCxAS_aK7sS2x_bqnlJHDSHw': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UCjwmbv6NE4mOh8Z8VhPUx1Q': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UCsP7Bpw36J666Fct5M8u-ZA': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UChBEbMKI1eCcejTtmI32UEw': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UCNbngWUqL2eqRw12yAwcICg': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UC-N2GFf6Dmna8NZ51vO--Lg': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UCRxAgfYexGLlu1WHGIMUDqw': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UCxD2E-bVoUbaVFL0Q3PvJTg': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UCcjhYlL1WRBjKaJsMH_h7Lg': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UCYjk_zY-iYR8YNfJmuzd70A': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UCpSgg_ECBj25s9moCDfSTsA': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UCvM1hVcRJmVWDtATYarC0KA': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UCmuK_ntZEGCXOmnfZW_LWnw': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UCb75CvYbm5BXpbEkGqFKABw': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UC1aJuxLHlw8bBV6mfCqVfog': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UCPzFLpOblZEaIx2lpym1l1A': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UCSbgQOPHnLhKMus6O4lFM2A': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UC5xAkS4828lDivq8cKFGSyw': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UCk-69xtxuCDuVF7CaaGeXQA': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UCpprBWvibvmOlI8yJOEAAjA': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UC8gFadPgK2r1ndqLI04Xvvw': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UCbULqc7U1mCHiVSCIkwEpxw': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UCSuT9FSddzI6W5Bij9XwtmA': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UCYidQwKhM3WTDKpT8pwfJzw': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UCyEA3vUnlpg0xzkECEq1rOA': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UC7eLtGAzNECUqurqMdiNYJg': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UCbyjYRUS9gADwAGuUxcfVcA': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UC1rIOwTqDuWkFj87HZYRFOg': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UC_8x1VmhDgsU72Yktd9Ukeg': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UCaLfMkkHhSA_LaCta0BzyhQ': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UCdTLhjAGVI_XDAuXrobtI6Q': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: [],\n",
       " 'UCsdD3quGf01RWABJt8wLe9g': Empty DataFrame\n",
       " Columns: [Comment Thread ID, Video ID, Top Level Comment, Total Replies, Can Reply, Replies]\n",
       " Index: []}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sheet_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f665ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('YoutubeCookingData')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "9e2bc71363a80f6dfd9bf347783d4f0752550c118317063593bfebf5e09d87b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
