{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "19517291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import googleapiclient.discovery as gapi\n",
    "from googleapiclient.errors import HttpError\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from datetime import date\n",
    "\n",
    "# TODO add some version of quota usage to estimate completion time, add some global variable that increments with each request depending on the cost of that method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acae34e",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3c1d1518",
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_FOLDER = 'C:\\\\Coding Projects\\\\YoutubeCookingData\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae3b28a",
   "metadata": {},
   "source": [
    "# Final Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "131be543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setAPI(apiKey):\n",
    "    return gapi.build('youtube', 'v3', developerKey=apiKey)\n",
    "\n",
    "def jsonFieldHandler(json, key1, key2='', key3='', key4=''):\n",
    "    # If the key doesn't exist, return ''\n",
    "    try:\n",
    "        if(key1 and key2 and key3 and key4):\n",
    "            return json[key1][key2][key3][key4]\n",
    "        elif(key1 and key2 and key3):\n",
    "            return json[key1][key2][key3]\n",
    "        elif(key1 and key2):\n",
    "            return json[key1][key2]\n",
    "        elif(key1):\n",
    "            return json[key1]\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "# Gets the data for all channels and parses it\n",
    "def getChannelDataHandler(channel_id_list):\n",
    "    channel_dict_list = []\n",
    "    channel_response_json = []\n",
    "    \n",
    "    # Get all fields for Channel\n",
    "    for i in range(0, len(channel_id_list), 50):\n",
    "        channel_response_json += getChannelData(channel_id_list[i:i+50])\n",
    "        \n",
    "    # Parse response into dictionary\n",
    "    for result in channel_response_json:\n",
    "#         print(f\"LOG: Fetching Data for Channel ID {result['id']}\")\n",
    "        channel_dict = {}\n",
    "        \n",
    "        channel_dict['Channel ID'] = jsonFieldHandler(result, 'id')\n",
    "        channel_dict['Title'] = jsonFieldHandler(result, 'snippet','title')\n",
    "        channel_dict['Description'] = jsonFieldHandler(result, 'snippet','description')\n",
    "        channel_dict['URL'] = jsonFieldHandler(result, 'snippet','customUrl')\n",
    "        channel_dict['Channel Created Date'] = jsonFieldHandler(result, 'snippet','publishedAt')\n",
    "        channel_dict['Thumbnail URL'] = jsonFieldHandler(result, 'snippet', 'thumbnails', 'high', 'url')\n",
    "        channel_dict['Language'] = jsonFieldHandler(result, 'snippet', 'defaultLanguage')\n",
    "        channel_dict['Country'] = jsonFieldHandler(result, 'snippet', 'country')\n",
    "        channel_dict['Views'] = jsonFieldHandler(result, 'statistics', 'viewCount')\n",
    "        channel_dict['Subscriber Count'] = jsonFieldHandler(result, 'statistics', 'subscriberCount')\n",
    "        channel_dict['Video Count'] = jsonFieldHandler(result, 'statistics', 'videoCount')\n",
    "        channel_dict['Topics'] = jsonFieldHandler(result, 'topicDetails', 'topicIds')\n",
    "        channel_dict['Topic Categories'] = jsonFieldHandler(result, 'topicDetails', 'topicCategories')\n",
    "        channel_dict['Upload Playlist ID'] = jsonFieldHandler(result, 'contentDetails', 'relatedPlaylists', 'uploads')\n",
    "\n",
    "        channel_dict_list.append(channel_dict)\n",
    "        \n",
    "    return pd.DataFrame(channel_dict_list)\n",
    "\n",
    "# Gets all fields for a given list of channel\n",
    "def getChannelData(channel_id_list):\n",
    "    request = YOUTUBE_SERVICE.channels().list(\n",
    "        part=['id', 'snippet', 'statistics', 'topicDetails', 'contentDetails'],\n",
    "        id=channel_id_list,\n",
    "        maxResults = 50\n",
    "    )\n",
    "    \n",
    "    results = request.execute()\n",
    "    return results['items']\n",
    "\n",
    "# Returns the upload playlist for a list of channel ids\n",
    "def getUploadPlaylistsforChannelHandler(channel_id_list):\n",
    "    channel_response_json = []\n",
    "    upload_playlist_list = []\n",
    "    \n",
    "    # Get upload playlists\n",
    "    for i in range(0, len(channel_id_list), 50):\n",
    "        channel_response_json += getUploadPlaylistsforChannel(channel_id_list[i:i+50])\n",
    "    \n",
    "    # Get only the channel's upload playlist id\n",
    "    return [jsonFieldHandler(channel_json, 'contentDetails', 'relatedPlaylists', 'uploads') for channel_json in channel_response_json]\n",
    "\n",
    "# Gets upload playlist ID for a list of channel IDs\n",
    "def getUploadPlaylistsforChannel(channel_id_list):\n",
    "    request = YOUTUBE_SERVICE.channels().list(\n",
    "    part=['contentDetails'],\n",
    "        id=channel_id_list,\n",
    "        maxResults=50\n",
    "    )\n",
    "    \n",
    "    result = request.execute()\n",
    "    \n",
    "    return result['items']\n",
    "    \n",
    "    \n",
    "    \n",
    "# Function that returns all videoIds for a given channel's upload playlist\n",
    "def getVideoListForPlaylist(upload_playlist_id, page_token=''):\n",
    "    video_ids = []\n",
    "\n",
    "    t1 = time.time()\n",
    "    \n",
    "    request = YOUTUBE_SERVICE.playlistItems().list(\n",
    "        part=\"contentDetails\",\n",
    "        playlistId=upload_playlist_id,\n",
    "        maxResults=50,\n",
    "        pageToken=page_token\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        results = request.execute()\n",
    "        result_videos = results['items']\n",
    "    except HttpError as err:\n",
    "        appendToLog(HttpError)\n",
    "        raise err\n",
    "    \n",
    "    # If there is a next page then append videos to result array\n",
    "    if('nextPageToken' in results):\n",
    "        nextPage = results['nextPageToken']\n",
    "        result_videos += getVideoListForPlaylist(upload_playlist_id, nextPage)\n",
    "    \n",
    "    # If not called by another instance of this function, return only IDs\n",
    "    if(page_token == ''):\n",
    "#         print(f\"LOG: Working on Channel ID {channelId}.\")\n",
    "        t2 = time.time()\n",
    "        appendToLog(f'Time elapsed for videos of playlist {upload_playlist_id} {t2-t1} for {len(result_videos)} videos')\n",
    "        return [vid['contentDetails']['videoId'] for vid in result_videos]\n",
    "    else:\n",
    "        # If recursing, return whole response list\n",
    "        return result_videos\n",
    "\n",
    "# Function that parses the return JSON from getVideoListForPlaylist and gets additional columns\n",
    "def getVideoDataHandler(video_id_list):\n",
    "    video_response_json = []\n",
    "    video_dict_list = []\n",
    "    \n",
    "    # Get all fields for Video\n",
    "    for i in range(0, len(video_id_list), 50):\n",
    "        try:\n",
    "            video_response_json += getVideoData(video_id_list[i:i+50])\n",
    "        except HttpError as err:\n",
    "            raise err\n",
    "    \n",
    "    # Parse response into dictionary\n",
    "    for result in video_response_json:\n",
    "#         print(f\"\\tLOG: Fetching Data for Video ID {result['id']}\")\n",
    "        video_dict = {}\n",
    "        \n",
    "        video_dict['Title'] = jsonFieldHandler(result, 'snippet', 'title')\n",
    "        video_dict['Video ID'] = jsonFieldHandler(result, 'id')\n",
    "        video_dict['Channel ID'] = jsonFieldHandler(result, 'snippet', 'channelId')\n",
    "        video_dict['Duration'] = jsonFieldHandler(result, 'contentDetails', 'duration')\n",
    "        video_dict['Description'] = jsonFieldHandler(result, 'snippet', 'description')\n",
    "        video_dict['Publish Date'] = jsonFieldHandler(result, 'snippet', 'publishedAt')\n",
    "        video_dict['Thumbnail URL'] = jsonFieldHandler(result, 'snippet', 'thumbnails', 'high',  'url')\n",
    "        video_dict['View Count'] = jsonFieldHandler(result, 'statistics', 'viewCount')\n",
    "        video_dict['Like Count'] = jsonFieldHandler(result, 'statistics', 'likeCount')\n",
    "        video_dict['Comment Count'] = jsonFieldHandler(result, 'statistics', 'commentCount')        \n",
    "        video_dict['Video Definition'] = jsonFieldHandler(result, 'contentDetails', 'definition')\n",
    "        video_dict['Default Audio Language'] = jsonFieldHandler(result, 'snippet', 'defaultAudioLanguage')\n",
    "        video_dict['Tags'] = jsonFieldHandler(result, 'snippet', 'tags')\n",
    "        video_dict['Category ID'] = jsonFieldHandler(result, 'snippet', 'categoryId')\n",
    "        video_dict['Topic Details'] = jsonFieldHandler(result, 'topicDetails')\n",
    "        video_dict['Made for Kids'] = jsonFieldHandler(result, 'status', 'madeForKids')\n",
    "        video_dict['Favorite Count'] = jsonFieldHandler(result, 'statistics', 'favoriteCount')\n",
    "        video_dict_list.append(video_dict)\n",
    "        \n",
    "    return pd.DataFrame(video_dict_list)\n",
    "     \n",
    "def getVideoData(list_of_videos):\n",
    "    request = YOUTUBE_SERVICE.videos().list(\n",
    "        part=['contentDetails', 'liveStreamingDetails', 'id',\n",
    "              'snippet', 'statistics', 'status', 'topicDetails'],\n",
    "        id=list_of_videos,\n",
    "        maxResults=50,\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        results = request.execute()\n",
    "        return results['items']\n",
    "    except HttpError as err: \n",
    "        # If error, return \n",
    "        appendToLog(HttpError)\n",
    "        raise err\n",
    "\n",
    "# Gets the comment threads for all videos in a given list\n",
    "def getCommentThreadsHandler(list_of_videos):\n",
    "    dataframes_list = []\n",
    "    \n",
    "    for video_id in list_of_videos:\n",
    "        try:\n",
    "            dataframes_list.append(getCommentsForVideo(video_id))\n",
    "        except HttpError as err:\n",
    "            # If it is bad request, try again in 5 seconds. Else raise error\n",
    "            if(err.resp.status == 400):\n",
    "                try:\n",
    "                    appendToLog(f\"Potentially transient ({err.resp.status}), trying again for video {video_id} after 5 minutes.\")\n",
    "                    time.sleep(600)\n",
    "                    dataframes_list.append(getCommentsForVideo(video_id))\n",
    "                except HttpError as err:\n",
    "                    raise err\n",
    "            else:\n",
    "                raise err\n",
    "\n",
    "    return pd.concat(dataframes_list, ignore_index=True)\n",
    "\n",
    "# Get list of comment threads for a given video, returns a dataframe\n",
    "def getCommentsForVideo(video_id, page_token=\"\"):\n",
    "#     print(f\"\\tLOG: Fetching Comment Data for Video ID {video_id}, {page_token}\")\n",
    "    t1 = time.time()\n",
    "    request = YOUTUBE_SERVICE.commentThreads().list(\n",
    "        part=['id', 'replies', 'snippet'],\n",
    "        videoId=video_id,\n",
    "        maxResults=100,\n",
    "        pageToken=page_token\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        results = request.execute()\n",
    "        result_comments = results['items']\n",
    "    \n",
    "        if('nextPageToken' in results):\n",
    "            next_page = results['nextPageToken']\n",
    "            result_comments += getCommentsForVideo(video_id, next_page)\n",
    "        \n",
    "        # If not recursing, parse the full response\n",
    "        if(page_token == ''):\n",
    "            t2 = time.time()\n",
    "            appendToLog(f'Time elapsed for comments on video {video_id} {t2-t1} for {len(result_comments)} comments')\n",
    "            return parseCommentThreadResponse(result_comments)\n",
    "        else: \n",
    "            # Return raw results if called by another instance of this function\n",
    "            return result_comments\n",
    "    except HttpError as err:\n",
    "        # If this is the top recursion, print to log\n",
    "        if(page_token == ''):\n",
    "            appendToLog(f'{err.resp.status} - {err._get_reason()} - {video_id}')\n",
    "        raise err\n",
    "\n",
    "# Parse comment threads response JSON\n",
    "def parseCommentThreadResponse(list_of_threads):\n",
    "    thread_dict_list = []\n",
    "    \n",
    "    for thread in list_of_threads:\n",
    "#         print(f\"\\t\\tLOG: Fetching Data for Comment Thread ID {thread['id']}\")\n",
    "        thread_dict = {}\n",
    "        \n",
    "        thread_dict['Comment Thread ID'] = jsonFieldHandler(thread, 'id')\n",
    "        thread_dict['Video ID'] = jsonFieldHandler(thread, 'snippet', 'videoId')\n",
    "        thread_dict['Top Level Comment'] = jsonFieldHandler(thread, 'snippet', 'topLevelComment')\n",
    "        thread_dict['Total Replies'] = jsonFieldHandler(thread, 'snippet', 'totalReplyCount')\n",
    "        thread_dict['Can Reply'] = jsonFieldHandler(thread, 'snippet', 'canReply')\n",
    "        thread_dict['Replies'] = jsonFieldHandler(thread, 'replies')\n",
    "        \n",
    "        thread_dict_list.append(thread_dict)\n",
    "    \n",
    "    return pd.DataFrame(thread_dict_list)\n",
    "\n",
    "def appendToLog(message): \n",
    "    print(message)\n",
    "    current_time = datetime.datetime.now()\n",
    "    \n",
    "    file_path = CURRENT_FOLDER + '\\\\Logs\\\\' + f\"Log {current_time.year}-{current_time.month}-{current_time.day}.txt\"\n",
    "\n",
    "    if(os.path.exists(file_path)):\n",
    "        append_write = 'a'\n",
    "    else:\n",
    "        append_write ='w'\n",
    "    \n",
    "    with open(file_path, append_write) as f:\n",
    "        f.write(message + '\\n')\n",
    "\n",
    "def main(channels_flag=True, videos_flag=True, comments_flag=True, current_folder=''):\n",
    "    start_time = time.time()\n",
    "    appendToLog(f\"Starting execution for channel data: {channels_flag}, video data: {videos_flag}, comment data: {comments_flag}\")\n",
    "    CHANNEL_ID_TOTAL_DF = pd.read_csv(current_folder + \"Channel IDs.csv\")\n",
    "\n",
    "    # File Paths\n",
    "    channels_file_path = current_folder + \"Channel Data.csv\"\n",
    "    videos_file_path = current_folder + \"Video Data.csv\"\n",
    "    comments_file_path = current_folder + \"Comment Data Raw.csv\"\n",
    "    comment_progress_file_path = current_folder + \"Comment Progress.csv\"\n",
    "    \n",
    "    # If Channels step is done, skip\n",
    "    if(channels_flag):\n",
    "        # If the file exists check how many channels still need to be queried, else get the whole list\n",
    "        if(os.path.exists(channels_file_path)):\n",
    "            # Get list of channel ids that have been queried and compare against the total list of ids\n",
    "            channel_data_done_ids = pd.read_csv(channels_file_path)['Channel ID'].to_list()\n",
    "            \n",
    "            channelIdsToFetch = CHANNEL_ID_TOTAL_DF[~CHANNEL_ID_TOTAL_DF['ID'].isin(channel_data_done_ids)]['ID'].to_list()\n",
    "            appendToLog(f\"{len(channel_data_done_ids)} channels already done out of {CHANNEL_ID_TOTAL_DF['ID'].shape[0]}\")\n",
    "        else:\n",
    "            channelIdsToFetch = CHANNEL_ID_TOTAL_DF['ID'].to_list()\n",
    "            appendToLog(f\"No channels already done, querying all {len(channelIdsToFetch)} IDs\")\n",
    "        \n",
    "        # If there are channels to fetch, get data and write to csv\n",
    "        if(len(channelIdsToFetch) > 0):\n",
    "            channels_df = getChannelDataHandler(channelIdsToFetch)\n",
    "            # If there is an existing file, concat to avoid overwriting\n",
    "            if(os.path.exists(channels_file_path)):\n",
    "                    channels_df = pd.concat([channels_df, pd.read_csv(channels_file_path)], ignore_index=True)\n",
    "            channels_df.to_csv(channels_file_path, index=False)\n",
    "            appendToLog(f\"Done {channels_df.shape[0]} channels\")\n",
    "    \n",
    "    # Check video progress, skip if done\n",
    "    if(videos_flag):\n",
    "        video_ids = []\n",
    "        videos_list = []\n",
    "        # Check if a given channel has already had its videos fetched\n",
    "        if(os.path.exists(videos_file_path)):\n",
    "            channel_data_done_ids_videos = pd.read_csv(videos_file_path)['Channel ID'].unique().tolist()\n",
    "            \n",
    "            channel_ids_to_fetch_videos = CHANNEL_ID_TOTAL_DF[~CHANNEL_ID_TOTAL_DF['ID'].isin(channel_data_done_ids_videos)]['ID'].to_list()\n",
    "            playlist_ids_to_fetch_videos = getUploadPlaylistsforChannelHandler(channel_ids_to_fetch_videos)\n",
    "            appendToLog(f\"{len(channel_data_done_ids_videos)} channel's videos already done out of {len(CHANNEL_ID_TOTAL_DF['ID'].to_list())}\")\n",
    "            # Read in current data\n",
    "            videos_list.append(pd.read_csv(videos_file_path))\n",
    "        else:\n",
    "            channel_ids_to_fetch_videos = CHANNEL_ID_TOTAL_DF['ID'].to_list()\n",
    "            appendToLog(f\"No channel's videos already done, querying all {len(channel_ids_to_fetch_videos)} channels\")\n",
    "            playlist_ids_to_fetch_videos = getUploadPlaylistsforChannelHandler(channel_ids_to_fetch_videos)\n",
    "        for playlist_id in playlist_ids_to_fetch_videos:\n",
    "            try:\n",
    "                video_ids = getVideoListForPlaylist(playlist_id)\n",
    "                videos_list.append(getVideoDataHandler(video_ids))\n",
    "            except HttpError as err:\n",
    "                appendToLog(f\"Error caught while fetching video data, ending execution\")\n",
    "                # Write current data to file\n",
    "                if(os.path.exists(videos_file_path)):\n",
    "                    videos_list += pd.read_csv(videos_file_path)\n",
    "                pd.concat(videos_list, ignore_index=True).to_csv(videos_file_path, index=False)\n",
    "                return  \n",
    "\n",
    "        # If all videos are read successfully, write to file and continue\n",
    "        if(len(channel_ids_to_fetch_videos) > 0):\n",
    "            if(os.path.exists(videos_file_path)):\n",
    "                        videos_list += pd.read_csv(videos_file_path)\n",
    "            pd.concat(videos_list, ignore_index=True).to_csv(videos_file_path, index=False)\n",
    "            \n",
    "    if(comments_flag):\n",
    "        comments_list = []\n",
    "        if(os.path.exists(videos_file_path)):\n",
    "            videos_data_total_ids_df = pd.read_csv(videos_file_path)\n",
    "            if(os.path.exists(comment_progress_file_path)):\n",
    "                videos_data_done_ids_comments = pd.read_csv(comment_progress_file_path)['Video ID'].tolist()\n",
    "                # Only fetch videos that have comments\n",
    "                video_ids_to_fetch_comments = videos_data_total_ids_df[~videos_data_total_ids_df['Video ID'].isin(videos_data_done_ids_comments) \n",
    "                & videos_data_total_ids_df['Comment Count'] > 0]['Video ID'].to_list()\n",
    "                appendToLog(f\"Done {len(videos_data_done_ids_comments)} video's comments out of {videos_data_total_ids_df.shape[0]}\")\n",
    "            else:\n",
    "                # Only fetch videos that have comments\n",
    "                video_ids_to_fetch_comments = videos_data_total_ids_df[videos_data_total_ids_df['Comment Count'] > 0]['Video ID'].to_list()\n",
    "                appendToLog(f\"No comments done, getting data for all {len(video_ids_to_fetch_comments)} videos\")\n",
    "        else:\n",
    "            appendToLog(\"No video data file, exiting\")\n",
    "            return\n",
    "        for video_id in video_ids_to_fetch_comments:\n",
    "            try:\n",
    "                comments_list.append(getCommentThreadsHandler([video_id]))\n",
    "            except HttpError as err:\n",
    "                appendToLog(f\"Error caught while fetching comment data, ending execution\")\n",
    "                # Write current data to file \n",
    "                if(os.path.exists(comments_file_path)):\n",
    "                    comments_list += pd.read_csv(comments_file_path)\n",
    "                pd.concat(comments_list, ignore_index=True).to_csv(comments_file_path,index=False)\n",
    "                return\n",
    "        # Write each video's comments to the file if not excepted\n",
    "        if(len(video_ids_to_fetch_comments) > 0):\n",
    "            if(os.path.exists(comments_file_path)):\n",
    "                comments_list += pd.read_csv(comments_file_path)\n",
    "            pd.concat(comments_list, ignore_index=True).to_csv(comments_file_path, index=False)\n",
    "    end_time = time.time()\n",
    "    appendToLog(f\"Finished data gathering. Overall execution time: {end_time - start_time}\")\n",
    "        \n",
    "    return\n",
    "    \n",
    "# Takes bulk comment data and splits into subsheets, one for each channel\n",
    "def postProcessingComments(filePath, targetPath):\n",
    "\n",
    "    # If no work was done, return\n",
    "    if(not os.path.exists(filePath)):\n",
    "        appendToLog(\"No data found to process, exiting\")\n",
    "        return\n",
    "    \n",
    "    comment_data = pd.read_csv(filePath)\n",
    "    appendToLog(f\"Beginning comment post processing for {comment_data.shape[0]} comment threads.\")\n",
    "    videos_channels = pd.read_csv(\"Video Data.csv\")\n",
    "    video_channels = videos_channels[[\"Video ID\", \"Channel ID\"]]\n",
    "    done_videos_df = pd.DataFrame()\n",
    "\n",
    "    for channelId in video_channels['Channel ID'].unique():\n",
    "        temp_df = comment_data[comment_data['Video ID'].isin(video_channels[video_channels['Channel ID'] == channelId]['Video ID'])]\n",
    "\n",
    "        # If there was more than one comment thread found for the channel\n",
    "        if(temp_df.shape[0] > 1):\n",
    "            file_path = f\"{targetPath}{channelId}_comment_data.csv\"\n",
    "            # If the file exists, then read the current data drop duplicates and save with the new data. Else just create the file\n",
    "            if(os.path.exists(file_path)):\n",
    "                pd.concat([temp_df, pd.read_csv(file_path)], ignore_index=True).drop_duplicates().to_csv(file_path, index=False)\n",
    "            else:\n",
    "                temp_df.drop_duplicates().to_csv(file_path, index=False)\n",
    "            appendToLog(f\"Saved comment data for {channelId} in {file_path}\")\n",
    "            done_videos_df = pd.concat([done_videos_df, temp_df[['Video ID']]], ignore_index=True)\n",
    "            \n",
    "    \n",
    "    # Save videos that have been done to csv file for later reference\n",
    "    if(os.path.exists(\"Comment Progress.csv\")):\n",
    "        pd.concat([done_videos_df, pd.read_csv(\"Comment Progress.csv\")], ignore_index=True).drop_duplicates().to_csv(\"Comment Progress.csv\", index=False)\n",
    "    else:\n",
    "        done_videos_df.drop_duplicates().to_csv(\"Comment Progress.csv\", index=False)\n",
    "    os.remove(\"done_\" + filePath)\n",
    "    os.rename(filePath, \"done_\" + filePath)\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cc2eed45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on API key 0\n",
      "Starting execution for channel data: True, video data: True, comment data: True\n",
      "60 channels already done out of 60\n",
      "60 channel's videos already done out of 60\n",
      "Done 8208 video's comments out of 58334\n",
      "Time elapsed for comments on video l3Mb9RyW3V4 67.11434555053711 for 12141 comments\n",
      "Time elapsed for comments on video bKA06kWvo80 28.556307554244995 for 5663 comments\n",
      "No data found to process, exiting\n",
      "Working on API key 1\n",
      "Starting execution for channel data: True, video data: True, comment data: True\n",
      "60 channels already done out of 60\n",
      "60 channel's videos already done out of 60\n",
      "Done 8208 video's comments out of 58334\n"
     ]
    }
   ],
   "source": [
    "with open(\"apiKeys.txt\") as f:\n",
    "    for idx, key in enumerate(f.read().split('\\n')): \n",
    "        appendToLog(f\"API loaded for key {key[0:10]}XXXXXXXXXXXXXXXXXXXXXXXXXXXXX - Index: {idx}\")\n",
    "        YOUTUBE_SERVICE = setAPI(key)\n",
    "        try:\n",
    "            main(True, True, True, current_folder=CURRENT_FOLDER)\n",
    "        # If error thrown, process the comments and then proceed to increment the API\n",
    "        except:\n",
    "            postProcessingComments(\"Comment Data Raw.csv\", CURRENT_FOLDER + \"Comment Data\\\\\")\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ddc353",
   "metadata": {},
   "source": [
    "# Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f665ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((958360, 6), (1039465, 6), (8112, 2))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"done_Comment Data Raw.csv\").shape, pd.read_csv(\"Comment Data\\\\UCbRj3Tcy1Zoz3rcf83nW5kw_comment_data.csv\").shape, pd.read_csv(\"Comment Progress.csv\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c83dd09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('YoutubeCookingData')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "9e2bc71363a80f6dfd9bf347783d4f0752550c118317063593bfebf5e09d87b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
