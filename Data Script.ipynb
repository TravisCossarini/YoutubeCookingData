{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19517291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import googleapiclient.discovery as gapi\n",
    "from googleapiclient.errors import HttpError\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import concurrent.futures\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acae34e",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c1d1518",
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_FOLDER = 'C:\\\\Coding Projects\\\\YoutubeCookingData\\\\'\n",
    "\n",
    "with open(CURRENT_FOLDER + \"apiKeys.txt\") as f:\n",
    "    YOUTUBE_API_KEY = f.read()\n",
    "youtube_service = gapi.build('youtube', 'v3', developerKey=YOUTUBE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae3b28a",
   "metadata": {},
   "source": [
    "# Final Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "131be543",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "\n",
    "def jsonFieldHandler(json, key1, key2='', key3='', key4=''):\n",
    "    # If the key doesn't exist, return ''\n",
    "    try:\n",
    "        if(key1 and key2 and key3 and key4):\n",
    "            return json[key1][key2][key3][key4]\n",
    "        elif(key1 and key2 and key3):\n",
    "            return json[key1][key2][key3]\n",
    "        elif(key1 and key2):\n",
    "            return json[key1][key2]\n",
    "        elif(key1):\n",
    "            return json[key1]\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "# Gets the data for all channels and parses it\n",
    "def getChannelDataHandler(channel_id_list):\n",
    "    channel_dict_list = []\n",
    "    channel_response_json = []\n",
    "    \n",
    "    # Get all fields for Channel\n",
    "    for i in range(0, len(channel_id_list), 50):\n",
    "        channel_response_json += getChannelData(channel_id_list[i:i+50])\n",
    "        \n",
    "    # Parse response into dictionary\n",
    "    for result in channel_response_json:\n",
    "#         print(f\"LOG: Fetching Data for Channel ID {result['id']}\")\n",
    "        channel_dict = {}\n",
    "        \n",
    "        channel_dict['Channel ID'] = jsonFieldHandler(result, 'id')\n",
    "        channel_dict['Title'] = jsonFieldHandler(result, 'snippet','title')\n",
    "        channel_dict['Description'] = jsonFieldHandler(result, 'snippet','description')\n",
    "        channel_dict['URL'] = jsonFieldHandler(result, 'snippet','customUrl')\n",
    "        channel_dict['Channel Created Date'] = jsonFieldHandler(result, 'snippet','publishedAt')\n",
    "        channel_dict['Thumbnail URL'] = jsonFieldHandler(result, 'snippet', 'thumbnails', 'high', 'url')\n",
    "        channel_dict['Language'] = jsonFieldHandler(result, 'snippet', 'defaultLanguage')\n",
    "        channel_dict['Country'] = jsonFieldHandler(result, 'snippet', 'country')\n",
    "        channel_dict['Views'] = jsonFieldHandler(result, 'statistics', 'viewCount')\n",
    "        channel_dict['Subscriber Count'] = jsonFieldHandler(result, 'statistics', 'subscriberCount')\n",
    "        channel_dict['Video Count'] = jsonFieldHandler(result, 'statistics', 'videoCount')\n",
    "        channel_dict['Topics'] = jsonFieldHandler(result, 'topicDetails', 'topicIds')\n",
    "        channel_dict['Topic Categories'] = jsonFieldHandler(result, 'topicDetails', 'topicCategories')\n",
    "        channel_dict['Upload Playlist ID'] = jsonFieldHandler(result, 'contentDetails', 'relatedPlaylists', 'uploads')\n",
    "\n",
    "        channel_dict_list.append(channel_dict)\n",
    "        \n",
    "    return pd.DataFrame(channel_dict_list)\n",
    "\n",
    "# Gets all fields for a given list of channel\n",
    "def getChannelData(channel_id_list):\n",
    "    request = youtube_service.channels().list(\n",
    "        part=['id', 'snippet', 'statistics', 'topicDetails', 'contentDetails'],\n",
    "        id=channel_id_list,\n",
    "        maxResults = 50\n",
    "    )\n",
    "    \n",
    "    results = request.execute()\n",
    "    return results['items']\n",
    "\n",
    "# Returns the upload playlist for a list of channel ids\n",
    "def getUploadPlaylistsforChannelHandler(channel_id_list):\n",
    "    channel_response_json = []\n",
    "    upload_playlist_list = []\n",
    "    \n",
    "    # Get upload playlists\n",
    "    for i in range(0, len(channel_id_list), 50):\n",
    "        channel_response_json += getUploadPlaylistsforChannel(channel_id_list[i:i+50])\n",
    "    \n",
    "    # Get only the channel's upload playlist id\n",
    "    return [jsonFieldHandler(channel_json, 'contentDetails', 'relatedPlaylists', 'uploads') for channel_json in channel_response_json]\n",
    "\n",
    "# Gets upload playlist ID for a list of channel IDs\n",
    "def getUploadPlaylistsforChannel(channel_id_list):\n",
    "    request = youtube_service.channels().list(\n",
    "    part=['contentDetails'],\n",
    "        id=channel_id_list,\n",
    "        maxResults=50\n",
    "    )\n",
    "    \n",
    "    result = request.execute()\n",
    "    \n",
    "    return result['items']\n",
    "    \n",
    "    \n",
    "    \n",
    "# Function that returns all videoIds for a given channel's upload playlist\n",
    "def getVideoListForPlaylist(upload_playlist_id, page_token=''):\n",
    "    video_ids = []\n",
    "    \n",
    "    request = youtube_service.playlistItems().list(\n",
    "        part=\"contentDetails\",\n",
    "        playlistId=upload_playlist_id,\n",
    "        maxResults=50,\n",
    "        pageToken=page_token\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        results = request.execute()\n",
    "        result_videos = results['items']\n",
    "    except HttpError as err:\n",
    "        appendToLog(HttpError)\n",
    "        raise err\n",
    "    \n",
    "    # If there is a next page then append videos to result array\n",
    "    if('nextPageToken' in results):\n",
    "        nextPage = results['nextPageToken']\n",
    "        result_videos += getVideoListForPlaylist(upload_playlist_id, nextPage)\n",
    "    \n",
    "    # If not called by another instance of this function, return only IDs\n",
    "    if(page_token == ''):\n",
    "#         print(f\"LOG: Working on Channel ID {channelId}.\")\n",
    "        return [vid['contentDetails']['videoId'] for vid in result_videos]\n",
    "    else:\n",
    "        # If recursing, return whole response list\n",
    "        return result_videos\n",
    "\n",
    "# Function that parses the return JSON from getVideoListForPlaylist and gets additional columns\n",
    "def getVideoDataHandler(video_id_list):\n",
    "    video_response_json = []\n",
    "    video_dict_list = []\n",
    "    \n",
    "    # Get all fields for Video\n",
    "    for i in range(0, len(video_id_list), 50):\n",
    "        try:\n",
    "            video_response_json += getVideoData(video_id_list[i:i+50])\n",
    "        except HttpError as err:\n",
    "            raise err\n",
    "    \n",
    "    # Parse response into dictionary\n",
    "    for result in video_response_json:\n",
    "#         print(f\"\\tLOG: Fetching Data for Video ID {result['id']}\")\n",
    "        video_dict = {}\n",
    "        \n",
    "        video_dict['Title'] = jsonFieldHandler(result, 'snippet', 'title')\n",
    "        video_dict['Video ID'] = jsonFieldHandler(result, 'id')\n",
    "        video_dict['Channel ID'] = jsonFieldHandler(result, 'snippet', 'channelId')\n",
    "        video_dict['Duration'] = jsonFieldHandler(result, 'contentDetails', 'duration')\n",
    "        video_dict['Description'] = jsonFieldHandler(result, 'snippet', 'description')\n",
    "        video_dict['Publish Date'] = jsonFieldHandler(result, 'snippet', 'publishedAt')\n",
    "        video_dict['Thumbnail URL'] = jsonFieldHandler(result, 'snippet', 'thumbnails', 'high',  'url')\n",
    "        video_dict['View Count'] = jsonFieldHandler(result, 'statistics', 'viewCount')\n",
    "        video_dict['Like Count'] = jsonFieldHandler(result, 'statistics', 'likeCount')\n",
    "        video_dict['Comment Count'] = jsonFieldHandler(result, 'statistics', 'commentCount')        \n",
    "        video_dict['Video Definition'] = jsonFieldHandler(result, 'contentDetails', 'definition')\n",
    "        video_dict['Default Audio Language'] = jsonFieldHandler(result, 'snippet', 'defaultAudioLanguage')\n",
    "        video_dict['Tags'] = jsonFieldHandler(result, 'snippet', 'tags')\n",
    "        video_dict['Category ID'] = jsonFieldHandler(result, 'snippet', 'categoryId')\n",
    "        video_dict['Topic Details'] = jsonFieldHandler(result, 'topicDetails')\n",
    "        video_dict['Made for Kids'] = jsonFieldHandler(result, 'status', 'madeForKids')\n",
    "        video_dict['Favorite Count'] = jsonFieldHandler(result, 'statistics', 'favoriteCount')\n",
    "        video_dict_list.append(video_dict)\n",
    "        \n",
    "    return pd.DataFrame(video_dict_list)\n",
    "     \n",
    "def getVideoData(list_of_videos):\n",
    "    request = youtube_service.videos().list(\n",
    "        part=['contentDetails', 'liveStreamingDetails', 'id',\n",
    "              'snippet', 'statistics', 'status', 'topicDetails'],\n",
    "        id=list_of_videos,\n",
    "        maxResults=50,\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        results = request.execute()\n",
    "        return results['items']\n",
    "    except HttpError as err: \n",
    "        # If error, return \n",
    "        appendToLog(HttpError)\n",
    "        raise err\n",
    "\n",
    "# Gets the comment threads for all videos in a given list\n",
    "def getCommentThreadsHandler(list_of_videos):\n",
    "    dataframes_list = []\n",
    "    \n",
    "    for video_id in list_of_videos:\n",
    "        try:\n",
    "            dataframes_list.append(getCommentsForVideo(video_id))\n",
    "        except HttpError as err:\n",
    "            # If it is bad request, try again in 5 seconds. Else raise error\n",
    "            if(err.resp.status == '400'):\n",
    "                try:\n",
    "                    appendToLog(f\"Potentially transient ({err.resp.status}), trying again for video {video_id}\")\n",
    "                    time.sleep(600)\n",
    "                    dataframes_list.append(getCommentsForVideo(video_id))\n",
    "                except HttpError as err:\n",
    "                    raise err\n",
    "            else:\n",
    "                raise err\n",
    "\n",
    "    return pd.concat(dataframes_list)\n",
    "\n",
    "# Get list of comment threads for a given video, returns a dataframe\n",
    "def getCommentsForVideo(video_id, page_token=\"\"):\n",
    "#     print(f\"\\tLOG: Fetching Comment Data for Video ID {video_id}, {page_token}\")\n",
    "    t1 = time.time()\n",
    "    request = youtube_service.commentThreads().list(\n",
    "        part=['id', 'replies', 'snippet'],\n",
    "        videoId=video_id,\n",
    "        maxResults=100,\n",
    "        pageToken=page_token\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        results = request.execute()\n",
    "        result_comments = results['items']\n",
    "    \n",
    "        if('nextPageToken' in results):\n",
    "            next_page = results['nextPageToken']\n",
    "            result_comments += getCommentsForVideo(video_id, next_page)\n",
    "        \n",
    "        # If not recursing, parse the full response\n",
    "        if(page_token == ''):\n",
    "            t2 = time.time()\n",
    "            print(f'Time elapsed for comments on video {video_id} {t2-t1} for {len(result_comments)} comments')\n",
    "            return parseCommentThreadResponse(result_comments)\n",
    "        else: \n",
    "            # Return raw results if called by another instance of this function\n",
    "            return result_comments\n",
    "    except HttpError as err:\n",
    "        appendToLog(f'{err.resp.status} - {err._get_reason()} - {video_id} with page {page_token}')\n",
    "        raise err\n",
    "\n",
    "# Parse comment threads response JSON\n",
    "def parseCommentThreadResponse(list_of_threads):\n",
    "    thread_dict_list = []\n",
    "    \n",
    "    for thread in list_of_threads:\n",
    "#         print(f\"\\t\\tLOG: Fetching Data for Comment Thread ID {thread['id']}\")\n",
    "        thread_dict = {}\n",
    "        \n",
    "        thread_dict['Comment Thread ID'] = jsonFieldHandler(thread, 'id')\n",
    "        thread_dict['Video ID'] = jsonFieldHandler(thread, 'snippet', 'videoId')\n",
    "        thread_dict['Top Level Comment'] = jsonFieldHandler(thread, 'snippet', 'topLevelComment')\n",
    "        thread_dict['Total Replies'] = jsonFieldHandler(thread, 'snippet', 'totalReplyCount')\n",
    "        thread_dict['Can Reply'] = jsonFieldHandler(thread, 'snippet', 'canReply')\n",
    "        thread_dict['Replies'] = jsonFieldHandler(thread, 'replies')\n",
    "        \n",
    "        thread_dict_list.append(thread_dict)\n",
    "    \n",
    "    return pd.DataFrame(thread_dict_list)\n",
    "\n",
    "def appendToLog(message): \n",
    "    print(message)\n",
    "    current_time = datetime.datetime.now()\n",
    "    \n",
    "    file_path = CURRENT_FOLDER + f\"Log {current_time.year}-{current_time.month}-{current_time.day} at {current_time.hour}_{current_time.minute}.txt\"\n",
    "\n",
    "    if(os.path.exists(file_path)):\n",
    "        append_write = 'a'\n",
    "    else:\n",
    "        append_write ='w'\n",
    "    \n",
    "    with open(file_path, append_write) as f:\n",
    "        f.write(message + '\\n')\n",
    "\n",
    "def main(channels_flag=True, videos_flag=True, comments_flag=True, current_folder=''):\n",
    "#     CHANNEL_LIST = pd.read_csv(\"Channel IDs.csv\")['ID'].to_list()\n",
    "    appendToLog(f\"Starting execution for channel data: {channels_flag}, video data: {videos_flag}, comment data: {comments_flag}\")\n",
    "    CHANNEL_ID_TOTAL_DF = pd.read_csv(current_folder + \"Channel IDs.csv\")\n",
    "\n",
    "    # File Paths\n",
    "    channels_file_path = current_folder + \"Channel Data.csv\"\n",
    "    videos_file_path = current_folder + \"Video Data.csv\"\n",
    "    comments_file_path = current_folder + \"Comment Data.csv\"\n",
    "    \n",
    "    # If Channels step is done, skip\n",
    "    if(channels_flag):\n",
    "        # If the file exists check how many channels still need to be queried, else get the whole list\n",
    "        if(os.path.exists(channels_file_path)):\n",
    "            # Get list of channel ids that have been queried and compare against the total list of ids\n",
    "            channel_data_done_ids = pd.read_csv(channels_file_path)['Channel ID'].to_list()\n",
    "            \n",
    "            channelIdsToFetch = CHANNEL_ID_TOTAL_DF[~CHANNEL_ID_TOTAL_DF['ID'].isin(channel_data_done_ids)]['ID'].to_list()\n",
    "            appendToLog(f\"{len(channel_data_done_ids)} channels already done out of {CHANNEL_ID_TOTAL_DF['ID'].shape[0]}\")\n",
    "        else:\n",
    "            channelIdsToFetch = CHANNEL_ID_TOTAL_DF['ID'].to_list()\n",
    "            appendToLog(f\"No channels already done, querying all {len(channelIdsToFetch)} IDs\")\n",
    "        \n",
    "        # If there are channels to fetch, get data and write to csv\n",
    "        if(len(channelIdsToFetch) > 0):\n",
    "            channels_df = getChannelDataHandler(channelIdsToFetch)\n",
    "            channels_df.to_csv(channels_file_path)\n",
    "    \n",
    "    # Check video progress, skip if done\n",
    "    if(videos_flag):\n",
    "        video_ids = []\n",
    "        videos_list = []\n",
    "        # Check if a given channel has already had its videos fetched\n",
    "        if(os.path.exists(videos_file_path)):\n",
    "            channel_data_done_ids_videos = pd.read_csv(videos_file_path)['Channel ID'].unique().tolist()\n",
    "            \n",
    "            # TODO, switch this to load Upload Playlist IDs directly\n",
    "            channel_ids_to_fetch_videos = CHANNEL_ID_TOTAL_DF[~CHANNEL_ID_TOTAL_DF['ID'].isin(channel_data_done_ids_videos)]['ID'].to_list()\n",
    "            playlist_ids_to_fetch_videos = getUploadPlaylistsforChannelHandler(channel_ids_to_fetch_videos)\n",
    "            appendToLog(f\"{len(channel_data_done_ids_videos)} channel's videos already done out of {len(CHANNEL_ID_TOTAL_DF['ID'].to_list())}\")\n",
    "            # Read in current data\n",
    "            videos_list.append(pd.read_csv(videos_file_path))\n",
    "        else:\n",
    "            channel_ids_to_fetch_videos = CHANNEL_ID_TOTAL_DF['ID'].to_list()\n",
    "            appendToLog(f\"No channel's videos already done, querying all {len(channel_ids_to_fetch_videos)} channels\")\n",
    "            playlist_ids_to_fetch_videos = getUploadPlaylistsforChannelHandler(channel_ids_to_fetch_videos)\n",
    "        for playlist_id in playlist_ids_to_fetch_videos:\n",
    "            try:\n",
    "                video_ids = getVideoListForPlaylist(playlist_id)\n",
    "                print(len(video_ids), 1)\n",
    "                videos_list.append(getVideoDataHandler(video_ids))\n",
    "                print(len(video_ids), 2)\n",
    "            except HttpError as err:\n",
    "                print(len(video_ids), 3)\n",
    "                appendToLog(f\"Error caught while fetching video data, ending execution\")\n",
    "                # Write current data to file\n",
    "                pd.concat(videos_list).to_csv(videos_file_path)\n",
    "                return  \n",
    "\n",
    "        # If all videos are read successfully, write to file and continue\n",
    "        pd.concat(videos_list).to_csv(videos_file_path)\n",
    "            \n",
    "    if(comments_flag):\n",
    "        comment_ids = []\n",
    "        comments_list = []\n",
    "        if(os.path.exists(videos_file_path)):\n",
    "            videos_data_total_ids_df = pd.read_csv(videos_file_path)\n",
    "            if(os.path.exists(comments_file_path)):\n",
    "                videos_data_done_ids_comments = pd.read_csv(comments_file_path)['Video ID'].unique().tolist()\n",
    "                video_ids_to_fetch_comments = videos_data_total_ids_df[~videos_data_total_ids_df['Video ID'].isin(videos_data_done_ids_comments)]['Video ID'].to_list()\n",
    "                appendToLog(f\"Done {len(videos_data_done_ids_comments)} video's comments out of {videos_data_total_ids_df.shape[0]}\")\n",
    "                # Read in current data\n",
    "                comments_list.append(pd.read_csv(comments_file_path))\n",
    "            else:\n",
    "                video_ids_to_fetch_comments = videos_data_total_ids_df['Video ID'].to_list()\n",
    "                appendToLog(f\"No comments done, getting data for all {len(video_ids_to_fetch_comments)} videos\")\n",
    "        else:\n",
    "            appendToLog(\"No video data file, exiting\")\n",
    "            return\n",
    "        for video_id in video_ids_to_fetch_comments:\n",
    "            try:\n",
    "                print(len(comments_list), 1)\n",
    "                comments_list.append(getCommentThreadsHandler([video_id]))\n",
    "                print(len(comments_list), 2)\n",
    "            except HttpError as err:\n",
    "                appendToLog(f\"Error caught while fetching comment data, ending execution\")\n",
    "                print(len(comments_list), 3)\n",
    "                # Write current data to file \n",
    "                pd.concat(comments_list).to_csv(comments_file_path)\n",
    "                return\n",
    "        # Write each video's comments to the file if not excepted\n",
    "        pd.concat(comments_list).to_csv(comments_file_path)\n",
    "                \n",
    "    \n",
    "#     comments_df = getCommentThreadsHandler(video_ids)    \n",
    "#     comments_df.to_csv(comments_file_path)\n",
    "    \n",
    "#     return videos_df, comments_df\n",
    "    return\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc2eed45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting execution for channel data: True, video data: True, comment data: True\n",
      "3 channels already done out of 3\n",
      "3 channel's videos already done out of 3\n",
      "Done 802 video's comments out of 844\n",
      "1 1\n",
      "Time elapsed for comments on video exQ6oGefSiA 17.855167150497437 for 6289 comments\n",
      "2 2\n",
      "2 1\n",
      "Time elapsed for comments on video 7EnWiGYT1g4 63.70451045036316 for 18320 comments\n",
      "3 2\n",
      "3 1\n",
      "Time elapsed for comments on video 6PTuUMJ2Uh8 4.496380805969238 for 1132 comments\n",
      "4 2\n",
      "4 1\n",
      "Time elapsed for comments on video nEoSBL25RO4 6.02030611038208 for 1774 comments\n",
      "5 2\n",
      "5 1\n"
     ]
    }
   ],
   "source": [
    "main(current_folder=CURRENT_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ddc353",
   "metadata": {},
   "source": [
    "# Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c12f03c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1783881, 17)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"Comment Data.csv\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e4ec0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main(channels_flag=True, videos_flag=True, comments_flag=True, current_folder=''):\n",
    "# #     CHANNEL_LIST = pd.read_csv(\"Channel IDs.csv\")['ID'].to_list()\n",
    "#     CHANNEL_ID_TOTAL_DF = pd.read_csv(current_folder + \"Channel IDs.csv\")\n",
    "\n",
    "#     # File Paths\n",
    "#     channels_file_path = current_folder + \"Channel Data.csv\"\n",
    "#     videos_file_path = current_folder + \"Video Data.csv\"\n",
    "#     comments_file_path = current_folder + \"Comment Data.csv\"\n",
    "    \n",
    "#     # If Channels step is done, skip\n",
    "#     if(channels_flag):\n",
    "#         # If the file exists check how many channels still need to be queried, else get the whole list\n",
    "#         if(os.path.exists(channels_file_path)):\n",
    "#             # Get list of channel ids that have been queried and compare against the total list of ids\n",
    "#             channel_data_done_ids = pd.read_csv(channels_file_path)['Channel ID'].to_list()\n",
    "            \n",
    "#             channelIdsToFetch = CHANNEL_ID_TOTAL_DF[~CHANNEL_ID_TOTAL_DF['ID'].isin(channel_data_done_ids)]['ID'].to_list()\n",
    "#             appendToLog(f\"{len(channel_data_done_ids)} channels already done out of {CHANNEL_ID_TOTAL_DF['ID'].shape[0]}\")\n",
    "#         else:\n",
    "#             channelIdsToFetch = CHANNEL_ID_TOTAL_DF['ID'].to_list()\n",
    "#             appendToLog(f\"No channels already done, querying all {len(channelIdsToFetch)} IDs\")\n",
    "        \n",
    "#         # If there are channels to fetch, get data and write to csv\n",
    "#         if(len(channelIdsToFetch) > 0):\n",
    "#             channels_df = getChannelDataHandler(channelIdsToFetch)\n",
    "#             channels_df.to_csv(channels_file_path)\n",
    "    \n",
    "#     # Check video progress, skip if done\n",
    "#     if(videos_flag):\n",
    "#         video_ids = []\n",
    "#         videos_list = []\n",
    "#         # Check if a given channel has already had its videos fetched\n",
    "#         if(os.path.exists(videos_file_path)):\n",
    "#             channel_data_done_ids_videos = pd.read_csv(videos_file_path)['Channel ID'].unique().tolist()\n",
    "            \n",
    "#             # TODO, switch this to load Upload Playlist IDs directly\n",
    "#             channel_ids_to_fetch_videos = CHANNEL_ID_TOTAL_DF[~CHANNEL_ID_TOTAL_DF['ID'].isin(channel_data_done_ids_videos)]['ID'].to_list()\n",
    "#             playlist_ids_to_fetch_videos = getUploadPlaylistsforChannelHandler(channel_ids_to_fetch_videos)\n",
    "#             appendToLog(f\"{len(channel_data_done_ids_videos)} channel's videos already done out of {len(CHANNEL_ID_TOTAL_DF['ID'].to_list())}\")\n",
    "#             # Read in current data\n",
    "#             videos_list.append(pd.read_csv(videos_file_path))\n",
    "#         else:\n",
    "#             channel_ids_to_fetch_videos = CHANNEL_ID_TOTAL_DF['ID'].to_list()\n",
    "#             appendToLog(f\"No channel's videos already done, querying all {len(channel_ids_to_fetch_videos)} channels\")\n",
    "#             playlist_ids_to_fetch_videos = getUploadPlaylistsforChannelHandler(channel_ids_to_fetch_videos)\n",
    "#         for playlist_id in playlist_ids_to_fetch_videos:\n",
    "#             try:\n",
    "#                 video_ids = getVideoListForPlaylist(playlist_id)\n",
    "#                 print(len(videos_list), 1)\n",
    "#                 videos_list.append(getVideoDataHandler(video_ids))\n",
    "#                 print(len(videos_list), 2)\n",
    "#             except HttpError as err:\n",
    "#                 print(len(videos_list), 3)\n",
    "#                 appendToLog(f\"Error caught while fetching video data, ending execution\")\n",
    "#                 # Write current data to file\n",
    "#                 pd.concat(videos_list).to_csv(videos_file_path)\n",
    "#                 return  \n",
    "\n",
    "#         # If all videos are read successfully, write to file and continue\n",
    "#         pd.concat(videos_list).to_csv(videos_file_path)\n",
    "            \n",
    "#     if(comments_flag):\n",
    "#         comment_ids = []\n",
    "#         comments_list = []\n",
    "#         if(os.path.exists(videos_file_path)):\n",
    "#             videos_data_total_ids_df = pd.read_csv(videos_file_path)\n",
    "#             if(os.path.exists(comments_file_path)):\n",
    "#                 videos_data_done_ids_comments = pd.read_csv(comments_file_path)['Video ID'].unique().tolist()\n",
    "#                 video_ids_to_fetch_comments = videos_data_total_ids_df[~videos_data_total_ids_df['Video ID'].isin(videos_data_done_ids_comments)]['Video ID'].to_list()\n",
    "#                 appendToLog(f\"Done {len(videos_data_done_ids_comments)} video's comments out of {videos_data_total_ids_df.shape[0]}\")\n",
    "#                 # Read in current data\n",
    "#                 comments_list.append(pd.read_csv(comments_file_path))\n",
    "#             else:\n",
    "#                 video_ids_to_fetch_comments = videos_data_total_ids_df['Video ID'].to_list()\n",
    "#                 appendToLog(f\"No comments done, getting data for all {len(video_ids_to_fetch_comments)} videos\")\n",
    "#         else:\n",
    "#             appendToLog(\"No video data file, exiting\")\n",
    "#             return\n",
    "#         with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
    "#             future_to_url = {executor.submit(getCommentThreadsHandler, [video_id]) for video_id in video_ids_to_fetch_comments}\n",
    "\n",
    "#             for future in concurrent.futures.as_completed(future_to_url):\n",
    "#                 try:\n",
    "#                     print(len(comments_list), 1)\n",
    "#                     data = future.result()\n",
    "#                     comments_list.append(data)\n",
    "#                     print(len(comments_list), 2)\n",
    "#                 except:\n",
    "#                     appendToLog(f\"Error caught while fetching comment data, ending execution\")\n",
    "#                     print(len(comments_list), 3)\n",
    "#                     # Write current data to file \n",
    "#                     pd.concat(comments_list).to_csv(comments_file_path)\n",
    "#                     return\n",
    "#         # Write each video's comments to the file if not excepted\n",
    "#         pd.concat(comments_list).to_csv(comments_file_path)\n",
    "        \n",
    "                \n",
    "    \n",
    "# #     comments_df = getCommentThreadsHandler(video_ids)    \n",
    "# #     comments_df.to_csv(comments_file_path)\n",
    "    \n",
    "# #     return videos_df, comments_df\n",
    "#     return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('YoutubeCookingData')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "9e2bc71363a80f6dfd9bf347783d4f0752550c118317063593bfebf5e09d87b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
