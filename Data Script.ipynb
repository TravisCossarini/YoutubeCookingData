{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19517291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import googleapiclient.discovery as gapi\n",
    "from googleapiclient.errors import HttpError\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from datetime import date\n",
    "\n",
    "# TODO add some version of quota usage to estimate completion time, add some global variable that increments with each request depending on the cost of that method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acae34e",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c1d1518",
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_FOLDER = 'C:\\\\Coding Projects\\\\YoutubeCookingData\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae3b28a",
   "metadata": {},
   "source": [
    "# Final Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "131be543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setAPI(apiKey):\n",
    "    return gapi.build('youtube', 'v3', developerKey=apiKey)\n",
    "\n",
    "def jsonFieldHandler(json, key1, key2='', key3='', key4=''):\n",
    "    # If the key doesn't exist, return ''\n",
    "    try:\n",
    "        if(key1 and key2 and key3 and key4):\n",
    "            return json[key1][key2][key3][key4]\n",
    "        elif(key1 and key2 and key3):\n",
    "            return json[key1][key2][key3]\n",
    "        elif(key1 and key2):\n",
    "            return json[key1][key2]\n",
    "        elif(key1):\n",
    "            return json[key1]\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "# Gets the data for all channels and parses it\n",
    "def getChannelDataHandler(youtube_api, channel_id_list):\n",
    "    channel_dict_list = []\n",
    "    channel_response_json = []\n",
    "    \n",
    "    # Get all fields for Channel\n",
    "    for i in range(0, len(channel_id_list), 50):\n",
    "        channel_response_json += getChannelData(youtube_api, channel_id_list[i:i+50])\n",
    "        \n",
    "    # Parse response into dictionary\n",
    "    for result in channel_response_json:\n",
    "#         print(f\"LOG: Fetching Data for Channel ID {result['id']}\")\n",
    "        channel_dict = {}\n",
    "        \n",
    "        channel_dict['Channel ID'] = jsonFieldHandler(result, 'id')\n",
    "        channel_dict['Title'] = jsonFieldHandler(result, 'snippet','title')\n",
    "        channel_dict['Description'] = jsonFieldHandler(result, 'snippet','description')\n",
    "        channel_dict['URL'] = jsonFieldHandler(result, 'snippet','customUrl')\n",
    "        channel_dict['Channel Created Date'] = jsonFieldHandler(result, 'snippet','publishedAt')\n",
    "        channel_dict['Thumbnail URL'] = jsonFieldHandler(result, 'snippet', 'thumbnails', 'high', 'url')\n",
    "        channel_dict['Language'] = jsonFieldHandler(result, 'snippet', 'defaultLanguage')\n",
    "        channel_dict['Country'] = jsonFieldHandler(result, 'snippet', 'country')\n",
    "        channel_dict['Views'] = jsonFieldHandler(result, 'statistics', 'viewCount')\n",
    "        channel_dict['Subscriber Count'] = jsonFieldHandler(result, 'statistics', 'subscriberCount')\n",
    "        channel_dict['Video Count'] = jsonFieldHandler(result, 'statistics', 'videoCount')\n",
    "        channel_dict['Topics'] = jsonFieldHandler(result, 'topicDetails', 'topicIds')\n",
    "        channel_dict['Topic Categories'] = jsonFieldHandler(result, 'topicDetails', 'topicCategories')\n",
    "        channel_dict['Upload Playlist ID'] = jsonFieldHandler(result, 'contentDetails', 'relatedPlaylists', 'uploads')\n",
    "\n",
    "        channel_dict_list.append(channel_dict)\n",
    "        \n",
    "    return pd.DataFrame(channel_dict_list)\n",
    "\n",
    "# Gets all fields for a given list of channel\n",
    "def getChannelData(youtube_api, channel_id_list):\n",
    "    request = youtube_api.channels().list(\n",
    "        part=['id', 'snippet', 'statistics', 'topicDetails', 'contentDetails'],\n",
    "        id=channel_id_list,\n",
    "        maxResults = 50\n",
    "    )\n",
    "    \n",
    "    results = request.execute()\n",
    "    return results['items']\n",
    "\n",
    "# Returns the upload playlist for a list of channel ids\n",
    "def getUploadPlaylistsforChannelHandler(youtube_api, channel_id_list):\n",
    "    channel_response_json = []\n",
    "    \n",
    "    # Get upload playlists\n",
    "    for i in range(0, len(channel_id_list), 50):\n",
    "        channel_response_json += getUploadPlaylistsforChannel(youtube_api, channel_id_list[i:i+50])\n",
    "    \n",
    "    # Get only the channel's upload playlist id\n",
    "    return [jsonFieldHandler(channel_json, 'contentDetails', 'relatedPlaylists', 'uploads') for channel_json in channel_response_json]\n",
    "\n",
    "# Gets upload playlist ID for a list of channel IDs\n",
    "def getUploadPlaylistsforChannel(youtube_api, channel_id_list):\n",
    "    request = youtube_api.channels().list(\n",
    "    part=['contentDetails'],\n",
    "        id=channel_id_list,\n",
    "        maxResults=50\n",
    "    )\n",
    "    \n",
    "    result = request.execute()\n",
    "    \n",
    "    return result['items']\n",
    "    \n",
    "    \n",
    "    \n",
    "# Function that returns all videoIds for a given channel's upload playlist\n",
    "def getVideoListForPlaylist(youtube_api, upload_playlist_id, page_token=''):\n",
    "    video_ids = []\n",
    "\n",
    "    t1 = time.time()\n",
    "    \n",
    "    request = youtube_api.playlistItems().list(\n",
    "        part=\"contentDetails\",\n",
    "        playlistId=upload_playlist_id,\n",
    "        maxResults=50,\n",
    "        pageToken=page_token\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        results = request.execute()\n",
    "        result_videos = results['items']\n",
    "    except HttpError as err:\n",
    "        appendToLog(HttpError)\n",
    "        raise err\n",
    "    \n",
    "    # If there is a next page then append videos to result array\n",
    "    if('nextPageToken' in results):\n",
    "        nextPage = results['nextPageToken']\n",
    "        result_videos += getVideoListForPlaylist(youtube_api, upload_playlist_id, nextPage)\n",
    "    \n",
    "    # If not called by another instance of this function, return only IDs\n",
    "    if(page_token == ''):\n",
    "#         print(f\"LOG: Working on Channel ID {channelId}.\")\n",
    "        t2 = time.time()\n",
    "        appendToLog(f'Time elapsed for videos of playlist {upload_playlist_id} {t2-t1} for {len(result_videos)} videos')\n",
    "        return [vid['contentDetails']['videoId'] for vid in result_videos]\n",
    "    else:\n",
    "        # If recursing, return whole response list\n",
    "        return result_videos\n",
    "\n",
    "# Function that parses the return JSON from getVideoListForPlaylist and gets additional columns\n",
    "def getVideoDataHandler(youtube_api, video_id_list):\n",
    "    video_response_json = []\n",
    "    video_dict_list = []\n",
    "    \n",
    "    # Get all fields for Video\n",
    "    for i in range(0, len(video_id_list), 50):\n",
    "        try:\n",
    "            video_response_json += getVideoData(youtube_api, video_id_list[i:i+50])\n",
    "        except HttpError as err:\n",
    "            raise err\n",
    "    \n",
    "    # Parse response into dictionary\n",
    "    for result in video_response_json:\n",
    "#         print(f\"\\tLOG: Fetching Data for Video ID {result['id']}\")\n",
    "        video_dict = {}\n",
    "        \n",
    "        video_dict['Title'] = jsonFieldHandler(result, 'snippet', 'title')\n",
    "        video_dict['Video ID'] = jsonFieldHandler(result, 'id')\n",
    "        video_dict['Channel ID'] = jsonFieldHandler(result, 'snippet', 'channelId')\n",
    "        video_dict['Duration'] = jsonFieldHandler(result, 'contentDetails', 'duration')\n",
    "        video_dict['Description'] = jsonFieldHandler(result, 'snippet', 'description')\n",
    "        video_dict['Publish Date'] = jsonFieldHandler(result, 'snippet', 'publishedAt')\n",
    "        video_dict['Thumbnail URL'] = jsonFieldHandler(result, 'snippet', 'thumbnails', 'high',  'url')\n",
    "        video_dict['View Count'] = jsonFieldHandler(result, 'statistics', 'viewCount')\n",
    "        video_dict['Like Count'] = jsonFieldHandler(result, 'statistics', 'likeCount')\n",
    "        video_dict['Comment Count'] = jsonFieldHandler(result, 'statistics', 'commentCount')        \n",
    "        video_dict['Video Definition'] = jsonFieldHandler(result, 'contentDetails', 'definition')\n",
    "        video_dict['Default Audio Language'] = jsonFieldHandler(result, 'snippet', 'defaultAudioLanguage')\n",
    "        video_dict['Tags'] = jsonFieldHandler(result, 'snippet', 'tags')\n",
    "        video_dict['Category ID'] = jsonFieldHandler(result, 'snippet', 'categoryId')\n",
    "        video_dict['Topic Details'] = jsonFieldHandler(result, 'topicDetails')\n",
    "        video_dict['Made for Kids'] = jsonFieldHandler(result, 'status', 'madeForKids')\n",
    "        video_dict['Favorite Count'] = jsonFieldHandler(result, 'statistics', 'favoriteCount')\n",
    "        video_dict_list.append(video_dict)\n",
    "        \n",
    "    return pd.DataFrame(video_dict_list)\n",
    "     \n",
    "def getVideoData(youtube_api, list_of_videos):\n",
    "    request = youtube_api.videos().list(\n",
    "        part=['contentDetails', 'liveStreamingDetails', 'id',\n",
    "              'snippet', 'statistics', 'status', 'topicDetails'],\n",
    "        id=list_of_videos,\n",
    "        maxResults=50,\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        results = request.execute()\n",
    "        return results['items']\n",
    "    except HttpError as err: \n",
    "        # If error, return \n",
    "        appendToLog(HttpError)\n",
    "        raise err\n",
    "\n",
    "# Gets the comment threads for all videos in a given list\n",
    "def getCommentThreadsHandler(youtube_api, list_of_videos):\n",
    "    dataframes_list = []\n",
    "    \n",
    "    for video_id in list_of_videos:\n",
    "        try:\n",
    "            dataframes_list.append(getCommentsForVideo(youtube_api, video_id))\n",
    "        except HttpError as err:\n",
    "            # If it is bad request, try again in 5 seconds. Else raise error\n",
    "            if(err.resp.status == 400 or err.resp.status == 503):\n",
    "                try:\n",
    "                    appendToLog(f\"Potentially transient ({err.resp.status}), trying again for video {video_id} after 5 minutes.\")\n",
    "                    time.sleep(600)\n",
    "                    dataframes_list.append(getCommentsForVideo(youtube_api, video_id))\n",
    "                except HttpError as err:\n",
    "                    raise err\n",
    "            else:\n",
    "                raise err\n",
    "\n",
    "    return pd.concat(dataframes_list, ignore_index=True)\n",
    "\n",
    "# Get list of comment threads for a given video, returns a dataframe\n",
    "def getCommentsForVideo(youtube_api, video_id, page_token=\"\"):\n",
    "#     print(f\"\\tLOG: Fetching Comment Data for Video ID {video_id}, {page_token}\")\n",
    "    t1 = time.time()\n",
    "    request = youtube_api.commentThreads().list(\n",
    "        part=['id', 'replies', 'snippet'],\n",
    "        videoId=video_id,\n",
    "        maxResults=100,\n",
    "        pageToken=page_token\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        results = request.execute()\n",
    "        result_comments = results['items']\n",
    "    \n",
    "        if('nextPageToken' in results):\n",
    "            next_page = results['nextPageToken']\n",
    "            result_comments += getCommentsForVideo(youtube_api, video_id, next_page)\n",
    "        \n",
    "        # If not recursing, parse the full response\n",
    "        if(page_token == ''):\n",
    "            t2 = time.time()\n",
    "            appendToLog(f'Time elapsed for comments on video {video_id} {t2-t1} for {len(result_comments)} comments')\n",
    "            return parseCommentThreadResponse(result_comments)\n",
    "        else: \n",
    "            # Return raw results if called by another instance of this function\n",
    "            return result_comments\n",
    "    except HttpError as err:\n",
    "        # If this is the top recursion, print to log\n",
    "        if(page_token == ''):\n",
    "            appendToLog(f'{err.resp.status} - {err._get_reason()} - {video_id}')\n",
    "        raise err\n",
    "\n",
    "# Parse comment threads response JSON\n",
    "def parseCommentThreadResponse(list_of_threads):\n",
    "    thread_dict_list = []\n",
    "    \n",
    "    for thread in list_of_threads:\n",
    "#         print(f\"\\t\\tLOG: Fetching Data for Comment Thread ID {thread['id']}\")\n",
    "        thread_dict = {}\n",
    "        \n",
    "        thread_dict['Comment Thread ID'] = jsonFieldHandler(thread, 'id')\n",
    "        thread_dict['Video ID'] = jsonFieldHandler(thread, 'snippet', 'videoId')\n",
    "        thread_dict['Top Level Comment'] = jsonFieldHandler(thread, 'snippet', 'topLevelComment')\n",
    "        thread_dict['Total Replies'] = jsonFieldHandler(thread, 'snippet', 'totalReplyCount')\n",
    "        thread_dict['Can Reply'] = jsonFieldHandler(thread, 'snippet', 'canReply')\n",
    "        thread_dict['Replies'] = jsonFieldHandler(thread, 'replies')\n",
    "        \n",
    "        thread_dict_list.append(thread_dict)\n",
    "    \n",
    "    return pd.DataFrame(thread_dict_list)\n",
    "\n",
    "def appendToLog(message): \n",
    "    print(message)\n",
    "    current_time = datetime.datetime.now()\n",
    "    \n",
    "    file_path = CURRENT_FOLDER + '\\\\Logs\\\\' + f\"Log {current_time.year}-{current_time.month}-{current_time.day}.txt\"\n",
    "\n",
    "    if(os.path.exists(file_path)):\n",
    "        append_write = 'a'\n",
    "    else:\n",
    "        append_write ='w'\n",
    "    \n",
    "    with open(file_path, append_write) as f:\n",
    "        f.write(message + '\\n')\n",
    "\n",
    "def main(youtube_api, channels_flag=True, videos_flag=True, comments_flag=True, current_folder=''):\n",
    "    start_time = time.time()\n",
    "    appendToLog(f\"Starting execution for channel data: {channels_flag}, video data: {videos_flag}, comment data: {comments_flag}\")\n",
    "    CHANNEL_ID_TOTAL_DF = pd.read_csv(current_folder + \"Channel IDs.csv\")\n",
    "\n",
    "    # File Paths\n",
    "    channels_file_path = current_folder + \"Channel Data.csv\"\n",
    "    videos_file_path = current_folder + \"Video Data.csv\"\n",
    "    comments_file_path = current_folder + \"Comment Data Raw.csv\"\n",
    "    comment_progress_file_path = current_folder + \"Comment Progress.csv\"\n",
    "    \n",
    "    # If Channels step is done, skip\n",
    "    if(channels_flag):\n",
    "        # If the file exists check how many channels still need to be queried, else get the whole list\n",
    "        if(os.path.exists(channels_file_path)):\n",
    "            # Get list of channel ids that have been queried and compare against the total list of ids\n",
    "            channel_data_done_ids = pd.read_csv(channels_file_path)['Channel ID'].to_list()\n",
    "            \n",
    "            channelIdsToFetch = CHANNEL_ID_TOTAL_DF[~CHANNEL_ID_TOTAL_DF['ID'].isin(channel_data_done_ids)]['ID'].to_list()\n",
    "            appendToLog(f\"{len(channel_data_done_ids)} channels already done out of {CHANNEL_ID_TOTAL_DF['ID'].shape[0]}\")\n",
    "        else:\n",
    "            channelIdsToFetch = CHANNEL_ID_TOTAL_DF['ID'].to_list()\n",
    "            appendToLog(f\"No channels already done, querying all {len(channelIdsToFetch)} IDs\")\n",
    "        \n",
    "        # If there are channels to fetch, get data and write to csv\n",
    "        if(len(channelIdsToFetch) > 0):\n",
    "            channels_df = getChannelDataHandler(youtube_api, channelIdsToFetch)\n",
    "            # If there is an existing file, concat to avoid overwriting\n",
    "            if(os.path.exists(channels_file_path)):\n",
    "                    channels_df = pd.concat([channels_df, pd.read_csv(channels_file_path)], ignore_index=True)\n",
    "            channels_df.to_csv(channels_file_path, index=False)\n",
    "            appendToLog(f\"Done {channels_df.shape[0]} channels\")\n",
    "    \n",
    "    # Check video progress, skip if done\n",
    "    if(videos_flag):\n",
    "        video_ids = []\n",
    "        videos_list = []\n",
    "        # Check if a given channel has already had its videos fetched\n",
    "        if(os.path.exists(videos_file_path)):\n",
    "            channel_data_done_ids_videos = pd.read_csv(videos_file_path)['Channel ID'].unique().tolist()\n",
    "            \n",
    "            channel_ids_to_fetch_videos = CHANNEL_ID_TOTAL_DF[~CHANNEL_ID_TOTAL_DF['ID'].isin(channel_data_done_ids_videos)]['ID'].to_list()\n",
    "            playlist_ids_to_fetch_videos = getUploadPlaylistsforChannelHandler(youtube_api, channel_ids_to_fetch_videos)\n",
    "            appendToLog(f\"{len(channel_data_done_ids_videos)} channel's videos already done out of {len(CHANNEL_ID_TOTAL_DF['ID'].to_list())}\")\n",
    "            # Read in current data\n",
    "            videos_list.append(pd.read_csv(videos_file_path))\n",
    "        else:\n",
    "            channel_ids_to_fetch_videos = CHANNEL_ID_TOTAL_DF['ID'].to_list()\n",
    "            appendToLog(f\"No channel's videos already done, querying all {len(channel_ids_to_fetch_videos)} channels\")\n",
    "            playlist_ids_to_fetch_videos = getUploadPlaylistsforChannelHandler(youtube_api, channel_ids_to_fetch_videos)\n",
    "        for playlist_id in playlist_ids_to_fetch_videos:\n",
    "            try:\n",
    "                video_ids = getVideoListForPlaylist(youtube_api, playlist_id)\n",
    "                videos_list.append(getVideoDataHandler(youtube_api, video_ids))\n",
    "            except HttpError as err:\n",
    "                appendToLog(f\"Error caught while fetching video data, ending execution\")\n",
    "                # Write current data to file\n",
    "                pd.concat(videos_list, ignore_index=True).to_csv(videos_file_path, index=False)\n",
    "                return  \n",
    "\n",
    "        # If all videos are read successfully, write to file and continue\n",
    "        if(len(channel_ids_to_fetch_videos) > 0):\n",
    "            for i in videos_list:\n",
    "                print(i.shape)\n",
    "            pd.concat(videos_list, ignore_index=True).to_csv(videos_file_path, index=False)\n",
    "            \n",
    "    if(comments_flag):\n",
    "        comments_list = []\n",
    "        if(os.path.exists(videos_file_path)):\n",
    "            videos_data_total_ids_df = pd.read_csv(videos_file_path)\n",
    "            if(os.path.exists(comment_progress_file_path)):\n",
    "                videos_data_done_ids_comments = pd.read_csv(comment_progress_file_path)['Video ID'].tolist()\n",
    "                # Only fetch videos that have comments\n",
    "                video_ids_to_fetch_comments = videos_data_total_ids_df[~videos_data_total_ids_df['Video ID'].isin(videos_data_done_ids_comments) \n",
    "                & videos_data_total_ids_df['Comment Count'] > 0]['Video ID'].to_list()\n",
    "                appendToLog(f\"Done {len(videos_data_done_ids_comments)} video's comments out of {videos_data_total_ids_df.shape[0]}\")\n",
    "            else:\n",
    "                # Only fetch videos that have comments\n",
    "                video_ids_to_fetch_comments = videos_data_total_ids_df[videos_data_total_ids_df['Comment Count'] > 0]['Video ID'].to_list()\n",
    "                appendToLog(f\"No comments done, getting data for all {len(video_ids_to_fetch_comments)} videos\")\n",
    "        else:\n",
    "            appendToLog(\"No video data file, exiting\")\n",
    "            return\n",
    "        for video_id in video_ids_to_fetch_comments:\n",
    "            try:\n",
    "                comments_list.append(getCommentThreadsHandler(youtube_api, [video_id]))\n",
    "            except HttpError as err:\n",
    "                appendToLog(f\"Error caught while fetching comment data, ending execution\")\n",
    "                # Write current data to file \n",
    "                pd.concat(comments_list, ignore_index=True).to_csv(comments_file_path,index=False)\n",
    "                end_time = time.time() \n",
    "                appendToLog(f\"Finished data gathering. Overall execution time: {end_time - start_time}\")\n",
    "                return\n",
    "                \n",
    "        # Write each video's comments to the file if not excepted\n",
    "        if(len(video_ids_to_fetch_comments) > 0):\n",
    "            pd.concat(comments_list, ignore_index=True).to_csv(comments_file_path, index=False)\n",
    "    end_time = time.time()\n",
    "    appendToLog(f\"Finished data gathering. Overall execution time: {end_time - start_time}\")\n",
    "        \n",
    "    return\n",
    "    \n",
    "# Takes bulk comment data and splits into subsheets, one for each channel\n",
    "def postProcessingComments(filePath, targetPath):\n",
    "\n",
    "    # If no work was done, return\n",
    "    if(not os.path.exists(filePath)):\n",
    "        appendToLog(\"No data found to process, exiting\")\n",
    "        return\n",
    "    \n",
    "    comment_data = pd.read_csv(filePath)\n",
    "    appendToLog(f\"Beginning comment post processing for {comment_data.shape[0]} comment threads.\")\n",
    "    videos_channels = pd.read_csv(\"Video Data.csv\")\n",
    "    video_channels = videos_channels[[\"Video ID\", \"Channel ID\"]]\n",
    "    done_videos_df = pd.DataFrame()\n",
    "\n",
    "    for channelId in video_channels['Channel ID'].unique():\n",
    "        temp_df = comment_data[comment_data['Video ID'].isin(video_channels[video_channels['Channel ID'] == channelId]['Video ID'])]\n",
    "\n",
    "        # If there was more than one comment thread found for the channel\n",
    "        if(temp_df.shape[0] > 1):\n",
    "            file_path = f\"{targetPath}{channelId}_comment_data.csv\"\n",
    "            # If the file exists, then read the current data drop duplicates and save with the new data. Else just create the file\n",
    "            if(os.path.exists(file_path)):\n",
    "                pd.concat([temp_df, pd.read_csv(file_path)], ignore_index=True).drop_duplicates().to_csv(file_path, index=False)\n",
    "            else:\n",
    "                temp_df.drop_duplicates().to_csv(file_path, index=False)\n",
    "            appendToLog(f\"Saved comment data for {channelId} in {file_path}\")\n",
    "            done_videos_df = pd.concat([done_videos_df, temp_df[['Video ID']]], ignore_index=True)\n",
    "            \n",
    "    \n",
    "    # Save videos that have been done to csv file for later reference\n",
    "    if(os.path.exists(\"Comment Progress.csv\")):\n",
    "        pd.concat([done_videos_df, pd.read_csv(\"Comment Progress.csv\")], ignore_index=True).drop_duplicates().to_csv(\"Comment Progress.csv\", index=False)\n",
    "    else:\n",
    "        done_videos_df.drop_duplicates().to_csv(\"Comment Progress.csv\", index=False)\n",
    "    os.remove(\"done_\" + filePath)\n",
    "    os.rename(filePath, \"done_\" + filePath)\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc2eed45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API loaded for key AIzaSyAsOIXXXXXXXXXXXXXXXXXXXXXXXXXXXXX - Index: 0\n",
      "Starting execution for channel data: True, video data: True, comment data: True\n",
      "63 channels already done out of 63\n",
      "62 channel's videos already done out of 63\n",
      "Time elapsed for videos of playlist UUyEd6QBSgat5kkC6svyjudA 2.776261329650879 for 1268 videos\n",
      "(58975, 17)\n",
      "(1268, 17)\n",
      "Done 21124 video's comments out of 60243\n",
      "Time elapsed for comments on video OZX7pKintiI 0.13259100914001465 for 3 comments\n",
      "Time elapsed for comments on video WmblWZLztqA 0.13105082511901855 for 6 comments\n",
      "Time elapsed for comments on video A_2b-2asOzQ 0.13639044761657715 for 11 comments\n",
      "Time elapsed for comments on video w-kv1eKG4Ck 0.17616987228393555 for 61 comments\n",
      "Time elapsed for comments on video zZh4ir_Mqdc 0.2123408317565918 for 71 comments\n",
      "Time elapsed for comments on video HyQfDLGGAUg 0.3620786666870117 for 139 comments\n",
      "Time elapsed for comments on video 8O33YBIdWBw 0.19216179847717285 for 81 comments\n",
      "Time elapsed for comments on video RZ6fmDJ7cFA 0.3252243995666504 for 105 comments\n",
      "Time elapsed for comments on video sut4Pnzx-5g 0.44010043144226074 for 188 comments\n",
      "Time elapsed for comments on video ETYqslTDNOw 0.3565092086791992 for 115 comments\n",
      "Time elapsed for comments on video IP19X_XidNU 0.37795042991638184 for 170 comments\n",
      "Time elapsed for comments on video RhQzj7zaTAQ 0.37819910049438477 for 145 comments\n",
      "Time elapsed for comments on video LSQRHe6HVxw 0.34555530548095703 for 123 comments\n",
      "Time elapsed for comments on video NgZFNoxJYYg 0.36910200119018555 for 142 comments\n",
      "Time elapsed for comments on video DcyDotTZEaM 1.0428731441497803 for 460 comments\n",
      "Time elapsed for comments on video 9l5QXCPJA20 0.5850200653076172 for 237 comments\n",
      "Time elapsed for comments on video 0Hxc-rXtj1I 0.605766773223877 for 287 comments\n",
      "Time elapsed for comments on video r8bpwVXDeaQ 1.4834599494934082 for 580 comments\n",
      "Time elapsed for comments on video 6aMcU5WoFhM 0.4006001949310303 for 151 comments\n",
      "Time elapsed for comments on video wsD_mZgFpTA 1.240549087524414 for 325 comments\n",
      "Time elapsed for comments on video vUbQ_0KDkXY 1.388519287109375 for 531 comments\n",
      "Time elapsed for comments on video rKypXz-6rh0 1.985180139541626 for 425 comments\n",
      "Time elapsed for comments on video 14wC-hdHvNY 3.0301709175109863 for 814 comments\n",
      "Time elapsed for comments on video 0gR9nUfvkk4 1.193859338760376 for 313 comments\n",
      "Time elapsed for comments on video diDqh11XLg4 1.4612951278686523 for 339 comments\n",
      "Time elapsed for comments on video E2MvhpiKiZ0 0.9366395473480225 for 257 comments\n",
      "Time elapsed for comments on video OB9jAgtmKdA 0.6135272979736328 for 197 comments\n"
     ]
    }
   ],
   "source": [
    "with open(\"apiKeys.txt\") as f:\n",
    "    for i in range(5):\n",
    "        for idx, key in enumerate(f.read().split('\\n')): \n",
    "            appendToLog(f\"API loaded for key {key[0:10]}XXXXXXXXXXXXXXXXXXXXXXXXXXXXX - Index: {idx}\")\n",
    "            youtube_api = setAPI(key)\n",
    "            try:\n",
    "                main(youtube_api, True, True, True, current_folder=CURRENT_FOLDER)\n",
    "            # If error thrown, process the comments and then proceed to increment the API\n",
    "            except HttpError as err:\n",
    "                postProcessingComments(\"Comment Data Raw.csv\", CURRENT_FOLDER + \"Comment Data\\\\\")\n",
    "                pass\n",
    "        # Sleep for 7 hours\n",
    "        time.sleep(3600*7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ddc353",
   "metadata": {},
   "source": [
    "# Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f665ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 21124 video's comments\n"
     ]
    }
   ],
   "source": [
    "print(f'Done {pd.read_csv(\"Comment Progress.csv\").shape[0]} video\\'s comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c83dd09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21124, 2), (58334, 17))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"Comment Progress.csv\").shape, pd.read_csv(\"Video Data.csv\").shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b0c47de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60243, 17)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"Video Data.csv\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00076fa2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('YoutubeCookingData')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "9e2bc71363a80f6dfd9bf347783d4f0752550c118317063593bfebf5e09d87b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
